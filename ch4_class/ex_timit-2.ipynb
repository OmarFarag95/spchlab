{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neD2twCK41mW"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/compi1234/spchlab/blob/master/session3/timit-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiSK1LBqqxbL"
   },
   "source": [
    "## Classification of Speech Frames\n",
    "\n",
    "### PART IV:  RECOGNIZE A PHONEME FROM FILTERBANK FEATURES WITH A DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEBOGxM441mY"
   },
   "source": [
    "### 1. Setting up your Python Environment\n",
    " \n",
    "1. Import Python's Machine Learning Stack\n",
    "\n",
    "2. Import needed local utilities that are needed for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16446,
     "status": "ok",
     "timestamp": 1634903354628,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "r-5Fz07p41mZ",
    "outputId": "ca7c860f-cce5-4a34-d308-a639dccce3ec"
   },
   "outputs": [],
   "source": [
    "# Importing Python's baseline machine learning stack \n",
    "#\n",
    "%matplotlib inline\n",
    "import sys,os,io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import urllib.request\n",
    "\n",
    "# imports from the scikit-learn \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  ! pip install git+https://github.com/compi1234/pyspch.git\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "from pyspch.io.hillenbrand import fetch_hillenbrand, select_hillenbrand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634903354629,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "KabZnIBG41md"
   },
   "outputs": [],
   "source": [
    "# choose the colors you like :)\n",
    "palette = sns.color_palette(\"bright\")\n",
    "# palette=['red','green','blue','orange','brown','black','dodgerblue','mediumturquoise','cyan','violet','gold','salmon'] \n",
    "# sns.palplot(palette)\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIjZCTLg41mf"
   },
   "source": [
    "### 2. The Database \n",
    "The experiments in this notebook use a subset of the TIMIT database.\n",
    "Instead of using raw speech, we extract features from the speech signal.\n",
    "In what follows we will asses how \n",
    "There is FILTERBANK data from 3 vowels (i,a,uw) , 400 samples for training and 200 samples for testing.\n",
    "The data is 24-dimensional (24 channels in the filterbank).\n",
    "http://homes.esat.kuleuven.be/~spchlab/datasets/tinytimit/README.txt\n",
    "\n",
    "In the cell below the data is loaded into\n",
    "- data matrices   FBANK_train(2400,3), FBANK_test(600,3)\n",
    "- labels          y_train(2400,), y_test(600,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3580,
     "status": "ok",
     "timestamp": 1634903358200,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "hOHNVsK741mg"
   },
   "outputs": [],
   "source": [
    "# loads all data in a matlab file at given url to the contents structure\n",
    "# this is working for MATLAB 7.0 files and older ; not hdf5 MATLAB 7.3 or more recent\n",
    "def load_matlab_from_url(url):\n",
    "    url_response = urllib.request.urlopen(url)\n",
    "    matio = io.BytesIO(url_response.read())\n",
    "    contents = sio.loadmat(matio,squeeze_me=True)\n",
    "    return(contents)\n",
    "\n",
    "# we will import 400 train samples and 200 test samples for 3 vowels\n",
    "tinytimit = 'http://homes.esat.kuleuven.be/~spchlab/datasets/tinytimit/'\n",
    "url_mf = tinytimit + 'male-female.mat' \n",
    "data_mf = load_matlab_from_url(url_mf)\n",
    "url_vow3= tinytimit + 'a-i-uw-800.mat' \n",
    "data_vow3 = load_matlab_from_url(url_vow3)\n",
    "\n",
    "# labels\n",
    "y_train =np.full((2400,),'a',dtype='<U2')\n",
    "y_train[800:1600] =np.full((800,),'i',dtype='<U2')\n",
    "y_train[1600:2400] =np.full((800,),'uw',dtype='<U2')\n",
    "y_test =np.full((600,),'a',dtype='<U2')\n",
    "y_test[200:400] =np.full((200,),'i',dtype='<U2')\n",
    "y_test[400:600] =np.full((200,),'uw',dtype='<U2')\n",
    "classes = np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5hyWXNXqS6x"
   },
   "source": [
    "#### Different features of speech\n",
    "\n",
    "For our NN-based phoneme-classifire, we use three types of input features.First, we use the filterbank energies (FBANK) directly. Second, we use the PCA features derrived from the FBANK features. PCA transforms the FBANK feature such that more discr  simply changes the coordinate system will disentangle the most important dimensions in the FBANK fetaures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1634897167269,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "jZPRL9cY41mj"
   },
   "outputs": [],
   "source": [
    "# Different speech features\n",
    "\n",
    "# A. Filterbank Energies (spectral features)\n",
    "FB_train=data_vow3['ALLtrain'].T\n",
    "FB_test=data_vow3['ALLtest'].T\n",
    "\n",
    "# B. PCA features (Principle Component Analysis)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "T = pca.fit(FB_train)\n",
    "PCA_train = T.transform(FB_train)\n",
    "PCA_test = T.transform(FB_test)\n",
    "\n",
    "# C. Mel-Frequency Cepstral Coefficients\n",
    "MFCC_train = dct(FB_train, type=2, axis=1, norm='ortho')\n",
    "MFCC_test = dct(FB_test, type=2, axis=1, norm='ortho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdVlR9TSqNXq"
   },
   "source": [
    "#### Feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1634904310326,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "U7roBFLw7T8D"
   },
   "outputs": [],
   "source": [
    "# Visualize features \n",
    "\n",
    "# - assemble dataframes for easy plotting\n",
    "\n",
    "# A. Filterbank Energies (spectral features)\n",
    "dfX = pd.DataFrame(FB_train[:,0:4])\n",
    "dfy = pd.Series(y_train, name='vowel')\n",
    "FB_df = pd.concat([dfy, dfX], axis=1)\n",
    "\n",
    "# B. PCA features (Principle Component Analysis)\n",
    "dfX = pd.DataFrame(PCA_train[:,0:4])\n",
    "dfy = pd.Series(y_train, name='vowel')\n",
    "PCA_df = pd.concat([dfy, dfX], axis=1)\n",
    "\n",
    "# C. Mel-Frequency Cepstral Coefficients\n",
    "dfX = pd.DataFrame(MFCC_train[:,0:4])\n",
    "dfy = pd.Series(y_train, name='vowel')\n",
    "MFCC_df = pd.concat([dfy, dfX], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1634904312020,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "iBeYPHlx8UYu"
   },
   "outputs": [],
   "source": [
    "# choose feature to plot\n",
    "df = MFCC_df\n",
    "df.columns = [\"vowel\",\"0\",\"1\",\"2\",\"3\"]\n",
    "\n",
    "doPlot = False\n",
    "if doPlot:\n",
    "  # plot 2 first dimensions of feature \n",
    "  f=plt.figure(figsize=(10, 10))\n",
    "  sns.scatterplot(data=df, x=\"0\", y=\"1\", hue='vowel')\n",
    "\n",
    "  # plot all dimensions pairwise\n",
    "  g = sns.PairGrid(df, hue=\"vowel\")\n",
    "  g.map_diag(plt.hist, histtype=\"step\", linewidth=1)\n",
    "  g.map_offdiag(plt.scatter, s=1)\n",
    "  g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkQsiCUpDfzW"
   },
   "source": [
    "### Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1634897134228,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "GqEWBZMzs_NO"
   },
   "outputs": [],
   "source": [
    "#@title Neural network architecture {display-mode: \"form\"}\n",
    "\n",
    "# =============================================================================\n",
    "# Neural network architecure\n",
    "# =============================================================================\n",
    "\n",
    "# simple feedforward neural network \n",
    "class simple_ffnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, hidden_layer_sizes):\n",
    "        super(simple_ffnn, self).__init__()\n",
    "\n",
    "        # attributes\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "        # parameters\n",
    "        layer_sizes = (in_dim, *hidden_layer_sizes, out_dim)\n",
    "        layer_sizes_pairwise = [(layer_sizes[i], layer_sizes[i+1]) for \n",
    "                                 i in range(len(layer_sizes)-1)]\n",
    "\n",
    "        # define architecture\n",
    "        modulelist = nn.ModuleList([])\n",
    "        for layer_in_size, layer_out_size in layer_sizes_pairwise:\n",
    "            modulelist.append(nn.Linear(layer_in_size, layer_out_size))\n",
    "            modulelist.append(nn.Sigmoid())\n",
    "\n",
    "        # define network as nn.Sequential\n",
    "        self.net = nn.Sequential(*modulelist)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.net.apply(init_normal) \n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_uniform_(m.weight)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1634897135603,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "m2hFiMMK8hYZ"
   },
   "outputs": [],
   "source": [
    "#@title Dataset  {display-mode: \"form\"}\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset \n",
    "# =============================================================================\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"Simple dataset for easy sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, data_X, data_y, labels, labeldict, device):\n",
    "\n",
    "        # dimensionality\n",
    "        self.n_samples, self.n_features = data_X.shape\n",
    "        self.n_classes = len(labels)\n",
    "\n",
    "        # input data\n",
    "        self.frames = data_X # (n_samples, n_features)\n",
    "        self.frames = torch.as_tensor(self.frames, dtype=torch.float32).to(device)\n",
    "\n",
    "        # labels\n",
    "        if data_y.dtype != \"int64\":\n",
    "            data_y = np.vectorize(labeldict.get)(data_y)\n",
    "        self.labels = torch.as_tensor(data_y, dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx] \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return frame, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634897136311,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "QtEjU9PgsIlq"
   },
   "outputs": [],
   "source": [
    "#@title Neural network training {display-mode: \"form\"}\n",
    "\n",
    "# =============================================================================\n",
    "# Neural network training\n",
    "# =============================================================================\n",
    "\n",
    "# simple batch gradient descent \n",
    "def train_batch(network, train_X, train_y, criterion, optimizer,\n",
    "                device, n_epochs=500, every=50):\n",
    "\n",
    "    # data-format is torch tensor + send to device (ex. GPU)\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float32).to(device)\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long).to(device)\n",
    "\n",
    "    # send network to device (ex. GPU)\n",
    "    network.to(device)\n",
    "\n",
    "    # set network to training mode (vs. evaluation mode)\n",
    "    network.train() \n",
    "\n",
    "    # save training loss\n",
    "    train_loss = []\n",
    "\n",
    "    # train for some epochs - full batch\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(train_X)\n",
    "        loss = criterion(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%every == 0:\n",
    "          print('Epoch {}, loss {}'.format(epoch, loss.item()))\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "      \n",
    "    return train_loss\n",
    "\n",
    "# mini-batch gradient descent\n",
    "def train_minibatch(network, train_dl, criterion, optimizer, device,\n",
    "                    n_epochs=500, every=50):\n",
    "    \"\"\"\n",
    "    Makes use of torch dataloader (see later on) to split data into mini-batches\n",
    "    \"\"\" \n",
    "\n",
    "    #  per epoch: update network parameters \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # network to training mode\n",
    "        network.train()\n",
    "        \n",
    "        # save training loss\n",
    "        train_loss = []\n",
    "\n",
    "        # per mini-batch: compute gradient (backpropagation) + take step \n",
    "        running_loss = 0\n",
    "        steps = 0 \n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "\n",
    "            # mini-batch inputs and labels\n",
    "            frames, labels = data  \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize \n",
    "            outputs = network.net(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "\n",
    "            # running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        mean_minibatch_loss = running_loss/len(train_dl)\n",
    "        train_loss.append(mean_minibatch_loss)\n",
    "\n",
    "        if epoch%every == 0:   \n",
    "          print(\"Epoch %d -- av. loss per mini-batch %.2f\" % (epoch, mean_minibatch_loss))\n",
    "\n",
    "    return train_loss\n",
    "            \n",
    "# mini-batch gradient descent with early stopping\n",
    "def train_minibatch_earlystopping(network, train_dl, criterion, optimizer, device,\n",
    "                                  n_epochs=500, valid_X=None, valid_y=None, patience=5,\n",
    "                                  every=50):\n",
    "    \"\"\"\n",
    "    Makes use of torch dataloader (see later on) to split data into mini-batches\n",
    "    \"\"\" \n",
    "\n",
    "    # set early stopping counter\n",
    "    cnt_valid_loss_increase = 0\n",
    "\n",
    "    # data-format is torch tensor + send to device (ex. GPU)\n",
    "    valid_X = torch.tensor(valid_X, dtype=torch.float32).to(device)\n",
    "    valid_y = torch.tensor(valid_y, dtype=torch.long).to(device)\n",
    "\n",
    "    # save training and validation loss\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "\n",
    "    #  per epoch: update network parameters \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # network to training mode\n",
    "        network.train()\n",
    "        \n",
    "        # early stopping \n",
    "        earlystoppping = cnt_valid_loss_increase > patience\n",
    "        if earlystoppping:\n",
    "            print(\"stopped early after %d epochs\" % (epoch))\n",
    "            return train_loss_list, valid_loss_list\n",
    "\n",
    "        # per mini-batch: compute gradient (backpropagation) + take step \n",
    "        running_loss = 0\n",
    "        steps = 0 \n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "\n",
    "            # mini-batch inputs and labels\n",
    "            frames, labels = data  \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize \n",
    "            outputs = network.net(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "\n",
    "            # running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # training loss\n",
    "        mean_minibatch_loss = running_loss/len(train_dl)\n",
    "        train_loss_list.append(mean_minibatch_loss)\n",
    "        if epoch%every == 0:   \n",
    "            print(\"Epoch %d -- av. loss per mini-batch %.2f\" % (epoch, mean_minibatch_loss))\n",
    "\n",
    "        # validation loss\n",
    "        if valid_X is not None and valid_y is not None:          \n",
    "            valid_outputs = network.net(valid_X)\n",
    "            valid_loss = criterion(valid_outputs, valid_y)\n",
    "            valid_loss_list.append(valid_loss.item())\n",
    "            \n",
    "            # early stoppping\n",
    "            loss_increase = valid_loss_list[-1] > min(valid_loss_list[-patience:])\n",
    "            if loss_increase:\n",
    "                # no improvement compared to last 'patience' steps\n",
    "                cnt_valid_loss_increase += 1\n",
    "            else:\n",
    "                cnt_valid_loss_increase = 0\n",
    "\n",
    "    return train_loss_list, valid_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1634897137387,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "cNKJUp8Sr0Nt"
   },
   "outputs": [],
   "source": [
    "#@title Neural network evaluation {display-mode: \"form\"}\n",
    "\n",
    "# =============================================================================\n",
    "# Neural network evaluation\n",
    "# =============================================================================\n",
    "\n",
    "# evaluate criterion\n",
    "def evaluate_criterion(network, test_X, test_y, criterion, device):\n",
    "    \n",
    "    # data-format is torch tensor + send to device (ex. GPU)\n",
    "    test_X = torch.tensor(test_X, dtype=torch.float32).to(device)\n",
    "    test_y = torch.tensor(test_y, dtype=torch.long).to(device)\n",
    "\n",
    "    # send network to device (ex. GPU)\n",
    "    network.to(device)\n",
    "\n",
    "    # set network to evaluation mode (vs. training mode)\n",
    "    network.eval() \n",
    "\n",
    "    # compute loss based on criterion\n",
    "    pred_y = network.net(test_X)\n",
    "    loss = criterion(pred_y, test_y)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# evaluate confusion matrix\n",
    "def confusionmatrix(network, test_X, test_y, device):\n",
    "    \n",
    "    # data-format is torch tensor + send to device (ex. GPU)\n",
    "    test_X = torch.tensor(test_X, dtype=torch.float32).to(device)\n",
    "    test_y = torch.tensor(test_y, dtype=torch.long).to(device)\n",
    "\n",
    "    # send network to device (ex. GPU)\n",
    "    network.to(device)\n",
    "\n",
    "    # set network to evaluation mode (vs. training mode)\n",
    "    network.eval() \n",
    "\n",
    "    # compute confusion matrix\n",
    "    cm = np.zeros((network.out_dim,network.out_dim))\n",
    "    for input, label in zip(test_X, test_y):\n",
    "        prob = network.net(input) # output = posterior class probabilities\n",
    "        pred = torch.argmax(prob) # prediction = label (or neuron) with highest probability (One-Hot Encoding)\n",
    "        cm[label][pred] += 1\n",
    "    \n",
    "    return cm.astype(int)\n",
    "\n",
    "# evaluate phone error rate\n",
    "def evaluate_PER(confusionmatrix):\n",
    "\n",
    "    n_classes = confusionmatrix.shape[0]    \n",
    "    \n",
    "    # compute ER \n",
    "    trace = np.trace(confusionmatrix)\n",
    "    ER = 1- trace.sum() / confusionmatrix.sum()\n",
    "        \n",
    "    # compute ER per class (disregarding non label or not)\n",
    "    no_examples_pc = confusionmatrix.sum(axis=1)\n",
    "    ER_pc = [None] * n_classes\n",
    "    for i in range(n_classes):\n",
    "        if no_examples_pc[i] != 0:\n",
    "            ER_pc[i] = 1-confusionmatrix[i,i] / (no_examples_pc[i])\n",
    "        \n",
    "    return ER, ER_pc\n",
    "\n",
    "# =============================================================================\n",
    "# Confusion matrix plot\n",
    "# =============================================================================\n",
    "\n",
    "# pretty printing\n",
    "def plot_confusion_matrix(cm,labels=[],cmap=[]):\n",
    "    import seaborn as sns\n",
    "\n",
    "    if len(labels) == 0:\n",
    "        df_cm = pd.DataFrame(cm)\n",
    "    else:\n",
    "        df_cm = pd.DataFrame(cm, labels, labels)\n",
    "        \n",
    "    f,ax = plt.subplots(figsize=(6,6))\n",
    "    sns.set(font_scale=1.4)#for label size\n",
    "    sns.heatmap(df_cm, annot=True,fmt=\"d\",annot_kws={\"fontsize\": 14,\"color\":'k'},\n",
    "                square=True,linecolor='k',linewidth=1.5,cmap=cmap,cbar=False)\n",
    "    ax.tick_params(axis='y',labelrotation=0.0,left=True)\n",
    "    # font size\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLp0Vrj5sWdN"
   },
   "source": [
    "#### Experimental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1634904313703,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "rFQ6KEvMaDTR"
   },
   "outputs": [],
   "source": [
    "# Features (X) and labels (y) used for experiment\n",
    "\n",
    "# input data\n",
    "train_X = MFCC_train\n",
    "test_X = MFCC_test\n",
    "\n",
    "# define One-Hot Encoding for labels\n",
    "labels = [\"a\", \"i\", \"uw\"]\n",
    "labeldict = {\"a\" : 0, \"i\" : 1, \"uw\" : 2} \n",
    "inv_labeldict = {v : k for k, v in labeldict.items()}\n",
    "\n",
    "# more flexible label dictionairy (based on labels)\n",
    "# labels = [\"a\",\"i\",\"uw\",\"extra_label\"]\n",
    "# labeldict = {k : v for k, v in zip(labels, np.arange(len(labels)))}\n",
    "# inv_labeldict = {v : k for k, v in labeldict.items()}\n",
    "\n",
    "# encode labels\n",
    "train_y = np.vectorize(labeldict.get)(y_train)\n",
    "test_y = np.vectorize(labeldict.get)(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1634904317296,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "I0Tupeh0w2Xi"
   },
   "outputs": [],
   "source": [
    "# network dimensions\n",
    "in_dim = train_X.shape[1]\n",
    "out_dim = len(classes)\n",
    "hidden_layer_sizes = [512,512]\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q0emNBmtsd1"
   },
   "source": [
    "##### Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "elapsed": 36742,
     "status": "ok",
     "timestamp": 1634904355560,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "tKOzd3sxcDO5",
    "outputId": "368314e3-7cae-4c56-c3d7-fd769ccb9109"
   },
   "outputs": [],
   "source": [
    "# batch gradient descent\n",
    "batch_model = simple_ffnn(in_dim=in_dim, out_dim=out_dim, hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "# training setup\n",
    "n_epochs = 500\n",
    "batch_lrn_rate = 0.001\n",
    "batch_criterion = nn.CrossEntropyLoss()  # applies softmax()\n",
    "batch_optimizer = torch.optim.Adam(batch_model.parameters(), lr=batch_lrn_rate) # ties model-parameters to optimizer (back-propagation)\n",
    "\n",
    "# train network\n",
    "batch_train_loss= train_batch(batch_model, train_X, train_y, batch_criterion, batch_optimizer, device, n_epochs)\n",
    "\n",
    "# plot training loss\n",
    "plt.figure()\n",
    "plt.plot(batch_train_loss)\n",
    "plt.title(\"Training loss - batch gradient descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1634904477040,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "b4YYkT_urIXk",
    "outputId": "7855636d-31e3-455f-9097-341f2ee4229a"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusionmatrix(batch_model, test_X, test_y, device)\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Phone Error Rate (PER) + PER per phone class\n",
    "per, per_pc = evaluate_PER(cm)\n",
    "print(\"PER %.2f and PER per phone class %s\" % (per, np.round(per_pc, 4)))\n",
    "\n",
    "# Cross-entropy loss\n",
    "print(\"CEL %.2f\" % evaluate_criterion(batch_model, test_X, test_y, batch_criterion, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mksQj9lmxFBV"
   },
   "source": [
    "##### Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1634904480192,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "Ovbiy5MGyC77"
   },
   "outputs": [],
   "source": [
    "# Dataset and Dataloader for easy sampling mini-batches\n",
    "\n",
    "# construct dataset\n",
    "train_ds = SimpleDataset(train_X, train_y, labels, labeldict, device)\n",
    "\n",
    "# construct dataloader\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 70780,
     "status": "ok",
     "timestamp": 1634904552932,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "yNIhufu1w4Vj",
    "outputId": "1101c59f-c008-428c-e2a6-7d6d1db0f5ca"
   },
   "outputs": [],
   "source": [
    "# mini-batch gradient descent\n",
    "minibatch_model = simple_ffnn(in_dim=in_dim, out_dim=out_dim, hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "# training setup\n",
    "n_epochs = 200\n",
    "mb_lrn_rate = 0.001\n",
    "mb_criterion = nn.CrossEntropyLoss()  # applies softmax()\n",
    "mb_optimizer = torch.optim.Adam(minibatch_model.parameters(), lr=mb_lrn_rate) # ties model-parameters to optimizer (back-propagation)\n",
    "\n",
    "# train network\n",
    "mb_train_loss = train_minibatch(minibatch_model, train_dl, mb_criterion, mb_optimizer, device, n_epochs)\n",
    "\n",
    "# plot training loss\n",
    "plt.figure()\n",
    "plt.plot(mb_train_loss)\n",
    "plt.title(\"Training loss - batch gradient descent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 137912,
     "status": "ok",
     "timestamp": 1607958619483,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "",
      "userId": "09704130763050227218"
     },
     "user_tz": -60
    },
    "id": "0b56P9BGxbGL",
    "outputId": "046994ca-0a14-4d2d-cffa-7bed9a1f7df2"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusionmatrix(minibatch_model, test_X, test_y, device)\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Phone Error Rate (PER) + PER per phone class\n",
    "per, per_pc = evaluate_PER(cm)\n",
    "print(\"PER %.2f and PER per phone class %s\" % (per, np.round(per_pc, 4)))\n",
    "\n",
    "# Cross-entropy loss\n",
    "print(\"CEL %.2f\" % evaluate_criterion(minibatch_model, test_X, test_y, batch_criterion, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51MbqnIWrCbU"
   },
   "source": [
    "#### Mini-batch gradient descent with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "executionInfo": {
     "elapsed": 194128,
     "status": "ok",
     "timestamp": 1607958675705,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "",
      "userId": "09704130763050227218"
     },
     "user_tz": -60
    },
    "id": "an5DeiJUmePh",
    "outputId": "bcff36a9-d5b0-46ed-bc12-4f811dc767b5"
   },
   "outputs": [],
   "source": [
    "# test training method: mini-batch gradient descent\n",
    "minibatch_es_model = simple_ffnn(in_dim=in_dim, out_dim=out_dim, hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "# training setup\n",
    "n_epochs = 200\n",
    "patience = 30\n",
    "mb_es_lrn_rate = 0.0001\n",
    "mb_es_criterion = nn.CrossEntropyLoss()  # applies softmax()\n",
    "mb_es_optimizer = torch.optim.Adam(minibatch_es_model.parameters(), lr=mb_es_lrn_rate) # ties model-parameters to optimizer (back-propagation)\n",
    "\n",
    "# train network\n",
    "mb_es_train_loss, mb_es_valid_loss = train_minbatch_earlystopping(minibatch_es_model, train_dl, mb_es_criterion, mb_es_optimizer, device, n_epochs, test_X, test_y, patience)\n",
    "\n",
    "# plot training loss\n",
    "plt.figure()\n",
    "plt.plot(mb_es_train_loss)\n",
    "plt.plot(mb_es_valid_loss)\n",
    "plt.title(\"Training loss - batch gradient descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 194362,
     "status": "ok",
     "timestamp": 1607958675944,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "",
      "userId": "09704130763050227218"
     },
     "user_tz": -60
    },
    "id": "2QQnaL7poyAA",
    "outputId": "71665480-1c39-4cfa-de54-75fcedb3c9c8"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusionmatrix(minibatch_es_model, test_X, test_y, device)\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Phone Error Rate (PER) + PER per phone class\n",
    "per, per_pc = evaluate_PER(cm)\n",
    "print(\"PER %.2f and PER per phone class %s\" % (per, np.round(per_pc, 4)))\n",
    "\n",
    "# Cross-entropy loss\n",
    "print(\"CEL %.2f\" % evaluate_criterion(minibatch_es_model, test_X, test_y, batch_criterion, device))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tiny_timit_in_pytorch.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/compi1234/spchlab/blob/master/session3/timit-1.ipynb",
     "timestamp": 1601551199644
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
