{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eXAE50oHaiv"
   },
   "source": [
    "## Classification of Speech Frames\n",
    "\n",
    "### FROM FBANK, FBANK-PCA or CEPSTRAL FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cz9H6ylHaiw"
   },
   "source": [
    "### 1. Setting up your Python Environment\n",
    " \n",
    "1. Import all required Python Modules  \n",
    "  - Python's Baseline Machine Learning Stack       \n",
    "  - Specific utilities that are needed for this exercise     \n",
    "2. Set some default plotting and printing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHzo4XKfHaix"
   },
   "outputs": [],
   "source": [
    "# Import Python's baseline machine learning stack \n",
    "#   and some io modules\n",
    "% matplotlib inline\n",
    "import sys,os,io\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import urllib.request\n",
    "\n",
    "# import from the scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# import the DCT transform from scipy\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "# import our GaussianMixture Classifier  (install if needed from github)\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  ! pip install git+https://github.com/compi1234/spchlab.git\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "from spchutils.GaussianMixtureClf import GaussianMixtureClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0Rqdg72Hai2"
   },
   "outputs": [],
   "source": [
    "# just setting some plotting & formatting options, you can modify at will\n",
    "# choose the colors you like :)\n",
    "palette = sns.color_palette(\"bright\")\n",
    "# palette=['red','green','blue','orange','brown','black','dodgerblue','mediumturquoise','cyan','violet','gold','salmon'] \n",
    "sns.set_palette(palette)\n",
    "# sns.palplot(palette)\n",
    "\n",
    "# set precision in pandas for float outputs\n",
    "#pd.reset_option('display.float_format')\n",
    "pd.set_option('precision',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvCDw9f7Hai5"
   },
   "source": [
    "### 2. The Database \n",
    "The experiments in this notebook use a subset of the TIMIT database.\n",
    "There is MEL SCALED FILTERBANK data from 3 vowels (i,a,uw) , 800 samples for training and 200 samples for testing.\n",
    "The data is 24-dimensional containing the log energy in each of the 24 filterbank channels (each 1 mel wide).\n",
    "\n",
    "In the cell below the data is loaded into\n",
    "- data matrices   FBANK_train(2400,3), FBANK_test(600,3)\n",
    "- labels          y_train(2400,), y_test(600,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmwloDj0Hai5"
   },
   "outputs": [],
   "source": [
    "# loads all data in a matlab file at given url to the contents structure\n",
    "def load_matlab_from_url(url):\n",
    "    url_response = urllib.request.urlopen(url)\n",
    "    matio = io.BytesIO(url_response.read())\n",
    "    contents = sio.loadmat(matio,squeeze_me=True)\n",
    "    return(contents)\n",
    "#\n",
    "# we import 800 train samples and 200 test samples for 3 vowels\n",
    "tinytimit = 'http://homes.esat.kuleuven.be/~spchlab/datasets/tinytimit/'\n",
    "url_mf = tinytimit + 'male-female.mat' \n",
    "data_mf = load_matlab_from_url(url_mf)\n",
    "url_vow3= tinytimit + 'a-i-uw-800.mat' \n",
    "data_vow3 = load_matlab_from_url(url_vow3)\n",
    "#\n",
    "# we subtract a very rough normalization factor (as original data was not scaled before DFT)\n",
    "norm_fac = 40.0\n",
    "FB_train=data_vow3['ALLtrain'].T - norm_fac\n",
    "FB_test=data_vow3['ALLtest'].T - norm_fac\n",
    "y_train =np.full((2400,),'a',dtype='<U2')\n",
    "y_train[800:1600] =np.full((800,),'i',dtype='<U2')\n",
    "y_train[1600:2400] =np.full((800,),'uw',dtype='<U2')\n",
    "y_test =np.full((600,),'a',dtype='<U2')\n",
    "y_test[200:400] =np.full((200,),'i',dtype='<U2')\n",
    "y_test[400:600] =np.full((200,),'uw',dtype='<U2')\n",
    "classes = np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCaymJ66Hai8"
   },
   "source": [
    "### 3a. Data Exploration ... of filterbank data\n",
    "\n",
    "How to explore data in 24-dimensional space ?\n",
    "\n",
    "There is no trivial answer to this question.  \n",
    "First we try to visualize scatter plots of the first couple of filterbank energies\n",
    "\n",
    "Scatter plots may give us an insight as to which features are most relevant.\n",
    "We can make a simple scatter plot with 2 preselected features, or can have a conglomerate of scatter plots for a getter overview\n",
    "\n",
    "##### Tasks & Questions (1)\n",
    "1. What do the scatter plots tell you.  Are there features that must be retained or others that can surely be dropped ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU0G2FhbHai8"
   },
   "outputs": [],
   "source": [
    "# we use the sns.PairGrid method to construct a multitude of scatter plots over the first \n",
    "# filterbank coefficients\n",
    "dfX = pd.DataFrame(FB_train[:,0:4])\n",
    "dfy = pd.Series(y_train, name='vowel')\n",
    "df = pd.concat([dfy, dfX], axis=1)\n",
    "f=plt.figure(figsize=(10,10))\n",
    "g = sns.PairGrid(df,hue=\"vowel\")\n",
    "g.map_diag(plt.hist,histtype=\"step\", linewidth=1)\n",
    "g.map_offdiag(plt.scatter,s=1)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbOA-fKDHai_"
   },
   "source": [
    "### 3b. Data Exploration .. of projected data\n",
    "\n",
    "While there might be lots of information in this 24-dimensional data. It is not simple to observe it.\n",
    "Definitely it is not obvious in the first couple of filterbank coefficients.\n",
    "We definitely run into the limitations of 3-D visual representation.\n",
    "\n",
    "A common approach for a quick analysis is to project the data on its most important axes by Principal Component Analysis (PCA).   PCA just looks at the overall distribution of all data without taking the labels into account.  Hence it is an unsupervised method. \n",
    "We will not give any mathematical background here, but just think of looking in the best possible low-dimensional subspace of the full 24-dimensional space.\n",
    "\n",
    "In the gridplot below, we make scatterplots of different combinations of PCA coefficients; the bottom left hand side of the plots show the raw datapoints; the top right hand side shows density approximations.\n",
    "\n",
    "##### Tasks & Questions (2)\n",
    "1. What do these new scatter plots tell you?  Can you make a general statement about the distributions of PCA features ?\n",
    "2. In what units are the PCA-axis ?\n",
    "3. Any idea if the original filterbank features will perform better or worse than the PCA features ?\n",
    "4. If you may only use 2 PCA coefficients, which ones will you choose ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWNRDaliHajB"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "plt.close(f)\n",
    "pca = PCA()\n",
    "T = pca.fit(FB_train)\n",
    "PCA_train = T.transform(FB_train)\n",
    "PCA_test = T.transform(FB_test)\n",
    "dfX = pd.DataFrame(PCA_train[:,0:4])\n",
    "dfy = pd.Series(y_train, name='vowel')\n",
    "df = pd.concat([dfy, dfX], axis=1)\n",
    "#\n",
    "df.columns=[\"vowel\",\"pca0\",\"pca1\",\"pca2\",\"pca3\"]\n",
    "f=plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(data=df,x=\"pca0\",y=\"pca1\",hue='vowel')\n",
    "g = sns.PairGrid(df,hue=\"vowel\")\n",
    "# on the diagonal we can plot a histogram or a smooth kernel density plot\n",
    "#g.map_diag(plt.hist,histtype=\"step\", linewidth=1)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.map_offdiag(plt.scatter,s=2)\n",
    "g.map_upper(sns.kdeplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQs4yuiOHajE"
   },
   "outputs": [],
   "source": [
    "# we can also try to look in 3D ...\n",
    "# .. but that's the limit of our visual capacity and the data is still 24-D !\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(df.loc[0:200,\"pca0\"], df.loc[0:200,\"pca1\"], df.loc[0:200,\"pca2\"], c='blue', s=20)\n",
    "ax.scatter(df.loc[800:1000,\"pca0\"], df.loc[800:1000,\"pca1\"], df.loc[800:1000,\"pca2\"], c='orange', s=20)\n",
    "ax.scatter(df.loc[1600:1800,\"pca0\"], df.loc[1600:1800,\"pca1\"], df.loc[1600:1800,\"pca2\"], c='green', s=20)\n",
    "plt.legend(['a','i','uw'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qhjLPHCHajG"
   },
   "source": [
    "### 3c. Data Exploration .. MFCC features\n",
    "\n",
    "PCA is a very common technique in machine learning in general to achieve dimensionality reduction, especially when we are dealing with unsupersived settings. It is not commonly used in tasks where we have plenty of labeled data, as is the case for speech recognition.   In such circumstances two alternatives give consistently better performance: LDA or CEPSTRAL transformations.  LDA is generic, the cepstrum is known to work well with speech filterbank data.\n",
    "Both are linear transformations, similar to PCA, but conceptually quite different\n",
    "- PCA looks at the global data and gives a look along the most informative axes.  After transformation the global data is centered at the origin and almost normally distributed.\n",
    "- LDA is also a global transformation, however it takes class information into account in its optimization criterion.  LDA makes gross assumptions, namely that all classes are normally distributed with identical covariances.  Despite these unrealistic assumptions, LDA tends to  outperform PCA by a rather large margin in most machine learning tasks, including speech recognition. Another disadvantage of LDA is that it is learned from the labeled data, making it dependent on language, task and database.\n",
    "- The Cepstrum: is derived from the spectrum or filterbank energies via a DCT transform.  Its performance is close to LDA and has as great advantage that it is generic.  Also - almost magically - cepstral features are highly decorrelated and almost normally distributed making them very well suited for machine learning applications.   When the DCT is applied on a mel scaled spectrum (or mel filterbank energies) we call the output MEL FREQUENCY CEPSTRAL COEFFICIENTS (or MFCCs).   Finally they are very compact, making them \"cheap\" to work with. For all these reasons MFCCs have been a reference feature representation for speech recognition for decades.  While maybe not the best anymore, they are still very relevant for development, prototyping or embedded applications.\n",
    "\n",
    "\n",
    "MFCC's: Mel Frequency Cepstra\n",
    "=============================\n",
    "MFCC's are obtained by taking a DCT (discrete cosine transform) of log filterbank energies.\n",
    "The FBANK features we start from in this exercise are these log filterbank energies.\n",
    " \n",
    "In the first set of figures we compare side by side prototypical spectra with prototypical cepstra of our 3 vowels in the database.  You can turn on/off optional mean normalization such that the global distributions become zero-centered.  While the spectra are more intuitive, we will later see the power of the cepstral coefficients.  Note already \n",
    "that the FBANK's here are 24-Dimensional, while we limited the dimensionality of the cepstra to 13. \n",
    "\n",
    "The second set of figures shows scattered plots as with PCA features.\n",
    "\n",
    "##### Tasks & Questions (3)\n",
    "1. What do these new scatter plots tell you?\n",
    "2. In what units are the MFCC-axis ?\n",
    "3. Any preferred features by now ?\n",
    "4. Anything remarkable about the global variance of cepstral coefficients ? How would the plot of the prototyical cepstra look if we had applied mean **and** variance normalization ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRABw73fpnqs"
   },
   "outputs": [],
   "source": [
    "#  set MEAN_NORM to True if you want to display mean normalized values\n",
    "#  leave MEAN_NORM at False if you want to display raw values\n",
    "MEAN_NORM = False\n",
    "\n",
    "MFCC_train = dct(FB_train, type=2, axis=1, norm='ortho')\n",
    "MFCC_test = dct(FB_test, type=2, axis=1, norm='ortho')\n",
    "\n",
    "\n",
    "if MEAN_NORM:\n",
    "  FBmean = np.mean(FB_train,0)\n",
    "  MFCCmean = np.mean(MFCC_train,0)\n",
    "else:\n",
    "  FBmean = 0\n",
    "  MFCCmean = 0\n",
    "\n",
    "FB_typ_a = np.mean(FB_train[0:800,:],0) - FBmean\n",
    "FB_typ_i = np.mean(FB_train[800:1600,:],0) - FBmean\n",
    "FB_typ_uw = np.mean(FB_train[1600:2400,:],0) - FBmean\n",
    "MFCC_typ_a = np.mean(MFCC_train[0:800,:],0) - MFCCmean\n",
    "MFCC_typ_i = np.mean(MFCC_train[800:1600,:],0) -MFCCmean\n",
    "MFCC_typ_uw = np.mean(MFCC_train[1600:2400,:],0) -MFCCmean\n",
    "\n",
    "\n",
    "fig= plt.figure(figsize=(12,6))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(FB_typ_a,color='blue')\n",
    "ax.plot(FB_typ_i,color='red')\n",
    "ax.plot(FB_typ_uw,color='green')\n",
    "ax.legend(['a','i','uw'])\n",
    "ax.set_xlabel('Filterbank')\n",
    "ax.set_xlim(0,23)\n",
    "ax.set_ylabel('Log Energy')\n",
    "#plt.show()\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(MFCC_typ_a,color='blue')\n",
    "ax.plot(MFCC_typ_i,color='red')\n",
    "ax.plot(MFCC_typ_uw,color='green')\n",
    "ax.legend(['a','i','uw'])\n",
    "ax.set_xlabel('MFCC')\n",
    "ax.set_xlim(0,12)\n",
    "ax.set_ylabel('')\n",
    "if MEAN_NORM:\n",
    "  plt.suptitle(' Filterbank and MFCC values for vowels /a/,/i/,/uw/  (mean normalization per channel)')\n",
    "else:\n",
    "  plt.suptitle(' Filterbank and MFCC values for vowels /a/,/i/,/uw/  (raw values per channel)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARRdexy2HajG"
   },
   "outputs": [],
   "source": [
    "dfX = pd.DataFrame(MFCC_train[:,0:4])\n",
    "dfy = pd.Series(y_train, name='vowel')\n",
    "df = pd.concat([dfy, dfX], axis=1)\n",
    "df.columns=[\"vowel\",\"mfcc0\",\"mfcc1\",\"mfcc2\",\"mfcc3\"]\n",
    "#\n",
    "f=plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(data=df,x=\"mfcc1\",y=\"mfcc2\",hue='vowel')\n",
    "g = sns.PairGrid(df,hue=\"vowel\")\n",
    "g.map_diag(plt.hist,histtype=\"step\", linewidth=1)\n",
    "g.map_offdiag(plt.scatter,s=1)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_wy6quGHajJ"
   },
   "source": [
    "### 4. Classification using a Bayesian classifier and Gaussian models\n",
    "\n",
    "We will now explore the merit of the different representations in function of speech recognition performance.\n",
    "We use the class *GaussianMixtureClf*  as provided in *spchutils* .\n",
    "It is implemented as an extension to *sklearn*, using the same APIs and making use of the sklearn GaussianMixture density estimator.\n",
    "\n",
    "There are a number of parameters that you may adjust in the following code blocks\n",
    "- n_components:  the number of mixtures used in the Gaussian Mixture Model\n",
    "- n_dim: the dimension of the feature vector (should be less than 24 in all cases)\n",
    "- feature_type: \"MFCC\", \"FBANK\" or \"PCA\"\n",
    "- max_iter:  maximum number of iterations in EM algorithm \n",
    "\n",
    "##### Note\n",
    "You may get convergence warnings with certain settings.  It is probably an indication that you are trying to fit too many parameters with the available data.\n",
    "In this case it probably is an indication that n_components is too big for the available data\n",
    "\n",
    "##### Task & Questions (4)\n",
    "1. How important in n_dim ?\n",
    "2. How important is n_components ?\n",
    "3. How important is it in this case to do PCA first ?\n",
    "4. How similar is test performance on an independent test set when compared with predicted performance from train set ?  Do you see differences between the 3 different feature types?  Do you see differences with increasing feature dimension and number of components ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBRTSUiRHajJ"
   },
   "outputs": [],
   "source": [
    "# evaluation routine that returns accuracy on train and test set and confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,labels=[],cmap=[]):\n",
    "# note (08/11/2019): seaborn heatmaps in sns 0.9.0 have top and bottom cut off when using matplotlib 3.1.1\n",
    "# you can either downgrade matplotlib or install matplotlib 3.1.2 which should be available shortly\n",
    "\n",
    "    if len(labels) == 0:\n",
    "        df_cm = pd.DataFrame(cm)\n",
    "    else:\n",
    "        df_cm = pd.DataFrame(cm, labels, labels)\n",
    "        \n",
    "    f,ax = plt.subplots(figsize=(3,3))\n",
    "    sns.set(font_scale=1.4)#for label size\n",
    "    sns.heatmap(df_cm, annot=True,fmt=\"d\",annot_kws={\"fontsize\": 14,\"color\":'k'},\n",
    "                square=True,linecolor='k',linewidth=1.5,cmap=cmap,cbar=False)\n",
    "    ax.tick_params(axis='y',labelrotation=0.0,left=True)\n",
    "    # font size\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "def train_test_model(X_train,X_test,y_train,y_test,classes,\n",
    "                     n_components=1,max_iter=20,tol=1.e-3,print_result=True,print_cmat=False):\n",
    "    clf_GM = GaussianMixtureClf(classes=classes,n_components=n_components,max_iter=20,tol=1.e-3)\n",
    "    clf_GM.fit(X_train,y_train)\n",
    "    y_pred = clf_GM.predict(X_train)\n",
    "    acc_train = 100.0*skmetrics.accuracy_score(y_train, y_pred)\n",
    "    y_pred = clf_GM.predict(X_test)\n",
    "    acc_test = 100.0*skmetrics.accuracy_score(y_test, y_pred) \n",
    "    cmat = skmetrics.confusion_matrix(y_test,y_pred)\n",
    "    if(print_result):\n",
    "        lls, bics = llscore(clf_GM,X_train,y_train)\n",
    "        print('Training Set:  Accuracy = %.2f%%     LL = %.2f    BIC = %.2f ' % (acc_train,lls,bics) )\n",
    "        print('Test Set:      Accuracy = %.2f%%'  % (acc_test) )\n",
    "    if(print_cmat):\n",
    "        plot_confusion_matrix(cmat,labels=classes)\n",
    "    return (acc_test,acc_train)\n",
    "\n",
    "def llscore(GMM,X,y):\n",
    "    ''' Average log likelihood per sample over the full data set (X,y) \n",
    "    and BIC per sample '''\n",
    "    ll = 0.\n",
    "    for k in range(0,GMM.n_classes) :\n",
    "        ll += GMM.gmm[k].score(X[y== GMM.classes[k],: ])\n",
    "    lls = ll.mean()\n",
    "    nparam = ((2*n_dim+1)*n_components -1 ) * GMM.n_classes\n",
    "    bics = -2*lls + (np.log(X.shape[0])* nparam) / float(X.shape[0])\n",
    "    return(lls,bics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGAA3RetHajL"
   },
   "outputs": [],
   "source": [
    "# to analyze different configurations\n",
    "# change the parameters feature_type, n_dim or n_components in the following lines\n",
    "# if you want to exclue global energy (~ mfcc[0]), then adjust expressions for feature selection to  1:n_dim\n",
    "#\n",
    "feature_type = \"MFCC\"  # any of MFCC, FBANK or PCA\n",
    "n_dim = 13            #  feature dimensions to test\n",
    "n_components = 1      #  n_components to test\n",
    "#\n",
    "    \n",
    "X_train = eval(\"%s_train[:,0:n_dim]\"%feature_type)\n",
    "X_test = eval(\"%s_test[:,0:n_dim]\"%feature_type)\n",
    "print(\"*** feature_type=%s, dim=%d, n_components=%d ***\" % (feature_type,n_dim,n_components))\n",
    "_,_ = train_test_model(X_train,X_test,y_train,y_test,classes,n_components=n_components,print_result=True,print_cmat=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj9Oz0uKHajO"
   },
   "outputs": [],
   "source": [
    "# in this cell you can do a full scale analysis with multiple settings\n",
    "# it basically lets you iterate over the same parameters as above\n",
    "# and summarizes your results in a table\n",
    "\n",
    "\n",
    "# --------------\n",
    "feature_type_test = [\"FB\",\"PCA\",\"MFCC\"]   # any of MFCC, FB or PCA\n",
    "n_dim_test = [ 4, 8, 13, 24 ]            # list of feature dimensions to test\n",
    "n_components_test = [1]                    # list of n_components to test\n",
    "max_iter = 10                                # maximum number of iterations in the EM algorithm\n",
    "\n",
    "# --------------\n",
    "result_list=[]\n",
    "for feature_type in feature_type_test:\n",
    "    for n_components in n_components_test:\n",
    "        for n_dim in n_dim_test:  \n",
    "            X_train = eval(\"%s_train[:,0:n_dim]\"%feature_type)\n",
    "            X_test = eval(\"%s_test[:,0:n_dim]\"%feature_type)\n",
    "          \n",
    "            acc_test, acc_train = train_test_model(X_train,X_test,y_train,y_test,classes,\n",
    "                                n_components=n_components,print_result=False)\n",
    "            result_list.append({\"features\":feature_type,\"dim\":n_dim,\n",
    "                                \"n_c\":n_components,\"acc_train\":acc_train,\"acc_test\":acc_test})\n",
    "    \n",
    "df_list=pd.DataFrame(result_list,columns=[\"features\",\"dim\",\"n_c\",\"acc_train\",\"acc_test\"])\n",
    "df = df_list.set_index([\"features\",\"n_c\",\"dim\"])\n",
    "#pd.set_option('precision',2)\n",
    "#print(df.to_string())\n",
    "HTML(df.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xu70qlSfef8"
   },
   "source": [
    "### 5. Classification using a Neural Net\n",
    "\n",
    "Finally we compare our classification results using Gaussian Mixtures (GMM) with a Neural Net (NN) classifier.  For this application we may use the simplest of all neural nets: a multilayer perceptron (MLP).\n",
    "\n",
    "We have set acceptable training parameters for you.  The things we are interested in is the impact of different features (type and dimension) and the size of the network.\n",
    "\n",
    "##### Tasks & Questions (5)\n",
    "1. When you compare results from the GMM vs NN classifiers, what is the most striking difference that you observe ?\n",
    "2. How do you rank the different feature representations (FBANK vs. MFCC) ?  Is there a difference between the GMM and NN case ?\n",
    "3. How do you rank the different feature dimensions (let's say 13 sv. 24) ?  Any notable difference between the GMM and NN case ?\n",
    "4. After all the performed experiments, pls. judge the following statements\n",
    "  - MFCCs outperform FBANK features\n",
    "  - GMMs are better classifiers than NNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGLZlhrZHajQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmWn9ECXHajX"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ------ different parameters to be explored --------\n",
    "feature_type_test = [\"FBANK\", \"MFCC\"]  # any combination of MFCC, FBANK or PCA\n",
    "n_dim_test = [ 13, 24 ]   # list of feature dimensions to test\n",
    "hidden_layers_test = [(256) , (512) , (1024), (512,512) , (256,256,256) ]  # network configurations\n",
    "learning_rate_test = [ 0.005 ]   # initial learning rates for the network training\n",
    "\n",
    "# ----- extra parameters that can be changed, but might be fine for most tests\n",
    "max_iter = 500  # maximum number of iterations in the backpropagation algorithm\n",
    "momentum = 0.9  # only for SGD\n",
    "early_stopping = True\n",
    "validation_fraction = 0.1\n",
    "\n",
    "# --------------\n",
    "\n",
    "results = pd.DataFrame(columns=[\"features\",\"hidden layers\", \"lr\",\"train acc\",\"test acc\"])\n",
    "#results = []\n",
    "for feature_type in feature_type_test:\n",
    "  for n_dim in n_dim_test:\n",
    "    features = feature_type+\"(\"+str(n_dim)+\")\"\n",
    "    for hidden_layer_sizes in hidden_layers_test:\n",
    "        for learning_rate_init in learning_rate_test:\n",
    "            if feature_type == \"PCA\":\n",
    "                X_train = FB_pca_train[:,0:n_dim]   \n",
    "                X_test  = FB_pca_test[:,0:n_dim]    \n",
    "            elif feature_type == \"FBANK\":\n",
    "                X_train = FB_train[:,0:n_dim]   \n",
    "                X_test  = FB_test[:,0:n_dim]\n",
    "            elif feature_type == \"MFCC\":\n",
    "                X_train = MFCC_train[:,0:n_dim]   \n",
    "                X_test  = MFCC_test[:,0:n_dim]    \n",
    "\n",
    "            clf_MLP = MLPClassifier(solver='adam', \n",
    "                                    learning_rate_init=learning_rate_init,\n",
    "                                    hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                    max_iter=max_iter, alpha=1e-5, random_state=1,\n",
    "                                    early_stopping=early_stopping,\n",
    "                                    validation_fraction=validation_fraction)\n",
    "            clf_MLP.fit(X_train,y_train)\n",
    "\n",
    "            #print(\"*** feature_type(dim=%d), hidden layers=%s, lr=%.5f ***\" %       (n_dim,hidden_layer_sizes,learning_rate_init))\n",
    "            y_pred = clf_MLP.predict(X_train)\n",
    "            train_acc = 100.0*skmetrics.accuracy_score(y_train, y_pred)\n",
    "            # print('Train Set Accuracy: %.2f%%' % (100.0*train_acc))\n",
    "            y_pred = clf_MLP.predict(X_test)\n",
    "            test_acc = 100.0*skmetrics.accuracy_score(y_test, y_pred)\n",
    "            cmat = skmetrics.confusion_matrix(y_test,y_pred)\n",
    "\n",
    "            # 3. print and save results            \n",
    "            print('%s(dim=%d)  layers=%s   lr=%.5f     Train Accuracy:%.2f%%    Test Accuracy: %.2f%%' % (feature_type,n_dim,hidden_layer_sizes,learning_rate_init,train_acc,test_acc))\n",
    "            results = results.append({'features':features,\"hidden layers\":str(hidden_layer_sizes),'lr': learning_rate_init,'train acc': train_acc,'test acc': test_acc},ignore_index=True)\n",
    "    \n",
    "print_df = results.set_index([\"features\",\"hidden layers\",\"lr\"])\n",
    "HTML(print_df.to_html())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "timit-1.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/compi1234/spchlab/blob/master/session3/timit-1.ipynb",
     "timestamp": 1602774055829
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
