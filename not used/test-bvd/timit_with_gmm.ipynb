{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "timit_with_gmm",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Sg0pH041mX"
      },
      "source": [
        "## Classification of Speech Frames\n",
        "\n",
        "### PART IV:  RECOGNIZE A PHONEME FROM FILTERBANK FEATURES WITH A DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBOGxM441mY"
      },
      "source": [
        "### 1. Setting up your Python Environment\n",
        "\n",
        "1. Import Python's Machine Learning Stack and other utilities\n",
        "\n",
        "2. Quite a few helper routines are defined for learning our Neural Network in Pytorch. There is no need to go through this in detail, but interested people can of course do this. There are 3 blocks of code:\n",
        "\n",
        "  a. Auxiliary functions: data loading/saving + feature extraction + phone label mapping\n",
        "\n",
        "  b. Pytorch functions: NN architecture + training + evaluation \n",
        "\n",
        "  c. Define setups: predefined setups (feature extraction, labels, training data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-5Fz07p41mZ",
        "cellView": "form",
        "outputId": "7f38911e-92ab-4a04-a4c9-7069ef9014bc"
      },
      "source": [
        "#@title Import libraries\n",
        "\n",
        "%matplotlib inline\n",
        "import sys,os,io\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy.io as sio\n",
        "import urllib.request\n",
        "\n",
        "# imports from pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# reproducibility \n",
        "torch.manual_seed(0) \n",
        "np.random.seed(0)\n",
        "\n",
        "# imports from the scikit-learn \n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn import metrics as skmetrics\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.fftpack import dct\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  ! pip install git+https://github.com/compi1234/pyspch.git\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "from pyspch.GaussianMixtureClf import GaussianMixtureClf\n",
        "import pyspch.display as spchd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/compi1234/pyspch.git\n",
            "  Cloning https://github.com/compi1234/pyspch.git to /tmp/pip-req-build-n88wky35\n",
            "  Running command git clone -q https://github.com/compi1234/pyspch.git /tmp/pip-req-build-n88wky35\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (1.4.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (1.1.5)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (0.8.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (0.10.3.post1)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (3.2.2)\n",
            "Requirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.7/dist-packages (from pyspch==0.4.1) (7.6.5)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pyspch==0.4.1) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pyspch==0.4.1) (4.10.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pyspch==0.4.1) (5.1.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pyspch==0.4.1) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pyspch==0.4.1) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pyspch==0.4.1) (1.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pyspch==0.4.1) (3.5.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pyspch==0.4.1) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pyspch==0.4.1) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->pyspch==0.4.1) (2.1.9)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->pyspch==0.4.1) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->pyspch==0.4.1) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->pyspch==0.4.1) (1.0.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->pyspch==0.4.1) (0.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->pyspch==0.4.1) (21.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->pyspch==0.4.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->pyspch==0.4.1) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->pyspch==0.4.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->pyspch==0.4.1) (0.11.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pyspch==0.4.1) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pyspch==0.4.1) (4.9.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->pyspch==0.4.1) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->pyspch==0.4.1) (2018.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->pyspch==0.4.1) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->pyspch==0.4.1) (1.4.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.2.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.8.0->pyspch==0.4.1) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->pyspch==0.4.1) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->pyspch==0.4.1) (2.21)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.12.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->pyspch==0.4.1) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (4.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pyspch==0.4.1) (0.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->pyspch==0.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->pyspch==0.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->pyspch==0.4.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.8.0->pyspch==0.4.1) (3.0.4)\n",
            "Building wheels for collected packages: pyspch\n",
            "  Building wheel for pyspch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspch: filename=pyspch-0.4.1-py3-none-any.whl size=728011 sha256=771a78a9d261a3bf8f3906e3004b09dc27dc047fd8252092dbbc598ee884295f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-14to1omk/wheels/37/48/d3/ecbc5875edffd413745fe137437cdfebe588a3722fb8efc4d7\n",
            "Successfully built pyspch\n",
            "Installing collected packages: pydub, pyspch\n",
            "Successfully installed pydub-0.25.1 pyspch-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lVsE9SC5led",
        "cellView": "form"
      },
      "source": [
        "#@title Auxiliary functions \n",
        "\n",
        "## Pickle functions \n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# save pickled file\n",
        "def pickle_data(df, filename):   \n",
        "    file_object = gzip.open(filename, \"wb\")\n",
        "    print(\"Generating \" + filename, end=\" ...\")\n",
        "    pickle.dump(df, file_object)\n",
        "    print(\"done\")\n",
        "    file_object.close()\n",
        "\n",
        "# open pickled file\n",
        "def unpickle_data(filename):\n",
        "    file_object = open(filename, \"rb\")\n",
        "    df = pickle.load(file_object)\n",
        "    file_object.close()\n",
        "    return df\n",
        "\n",
        "\n",
        "## Download functions\n",
        "\n",
        "import requests\n",
        "\n",
        "# download from url and write to file\n",
        "def write_from_url(url, filename):\n",
        "    r = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "## Phoneme segmentation  \n",
        "\n",
        "# phone segmentation to phone per frame in utterance\n",
        "def phnseg_to_phns(phnseg, nframes):\n",
        "    phn_list = []\n",
        "    for seg_idx, seg in phnseg.iterrows():\n",
        "        seg_nframes = seg['t1'] - seg['t0']\n",
        "        phn_list += [seg['seg']] * seg_nframes\n",
        "        \n",
        "    return phn_list    \n",
        " \n",
        "# phone segmentation to center frame of phone per frame\n",
        "def phnseg_to_center_frames(phnseg, nframes):\n",
        "    center_frame_ids = []\n",
        "    for seg_idx, seg in phnseg.iterrows():\n",
        "        seg_nframes = seg['t1'] - seg['t0']\n",
        "        center_frame_ids += [int(seg['t0'] + seg_nframes / 2)] * seg_nframes\n",
        "        \n",
        "    return center_frame_ids\n",
        "\n",
        "##  Window\n",
        "\n",
        "# The window is determined by utterance boundaries (frame_idx, utt_nframes) \n",
        "# and the window parameters (window_nframes, stride). The window is represented \n",
        "# by indices relative to the frame index.\n",
        "\n",
        "# window indices relative to frame index\n",
        "def get_window(frame_idx, utt_nframes, window_nframes, stride, shift=0):\n",
        "    \n",
        "    def window_bounds(window_nframes):\n",
        "        if window_nframes % 2 == 0:    \n",
        "            # even case: start-i-center-i-i-end (example window_nframes=6)       \n",
        "            start = -window_nframes / 2 + 1\n",
        "            end = window_nframes / 2\n",
        "        else:                       \n",
        "            # uneven case: start-i-ref-i-stop (example window_nframes=5)       \n",
        "            start = -(window_nframes - 1) / 2\n",
        "            end = (window_nframes - 1) / 2  \n",
        "            \n",
        "        return start, end\n",
        "    \n",
        "    def adjust_window_to_utt_bounds(window):\n",
        "        new_window = []\n",
        "        for rel_idx in window:\n",
        "            rel_idx = max(rel_idx, 0 - frame_idx)\n",
        "            rel_idx = min(rel_idx, utt_nframes - 1 - frame_idx)\n",
        "            new_window.append(rel_idx)\n",
        "            \n",
        "        return np.array(new_window)\n",
        "    \n",
        "    # window boundaries w.r.t. center frame\n",
        "    start, end = window_bounds(window_nframes)\n",
        "    # window of evenly spaced frames around center frame\n",
        "    window = np.arange(start, end + 1).astype(int) \n",
        "    # apply stride and shift\n",
        "    window = window * stride + shift \n",
        "    # adjust window indices out of the utterance bounds\n",
        "    window = adjust_window_to_utt_bounds(window)\n",
        "    \n",
        "    return window\n",
        "\n",
        "# window indices for each frame in utterance\n",
        "def get_windows_for_utt(utt_nframes, window_nframes, stride, shift):\n",
        "    utt_windows = []\n",
        "    # iterate over frames in utterance\n",
        "    for frame_idx in range(utt_nframes):\n",
        "        window = get_window(frame_idx, utt_nframes,\n",
        "                            window_nframes, stride, shift)\n",
        "        utt_windows.append(window)\n",
        "    \n",
        "    return utt_windows\n",
        "\n",
        "# apply window to frames  \n",
        "def apply_window(frames, frame_ids, frame_windows):\n",
        "    inputs = []\n",
        "    # ensure type is numpy for easy indexing\n",
        "    frames = np.array(frames)\n",
        "    # iterate over frames \n",
        "    for frame_idx, frame_window in zip(frame_ids, frame_windows):\n",
        "        # apply window\n",
        "        feature = np.stack(frames[frame_window + int(frame_idx)], axis=0)\n",
        "        # save feature\n",
        "        inputs.append(feature)\n",
        "        \n",
        "    return np.stack(inputs, axis=0)\n",
        "\n",
        "## Reshape\n",
        "\n",
        "# reshape features assuming first dimension is 'examples'\n",
        "def reshape_features(frames, shape=(-1,)):\n",
        "    # n_examples = len(frames)\n",
        "    # new_shape = (n_examples, *shape)\n",
        "    \n",
        "    return [np.reshape(frame, shape) for frame in frames]\n",
        "\n",
        "## Normalization and derrivatives\n",
        "\n",
        "# normalization\n",
        "def normalize_data(data, axis=0, normMean=False, normVar=True):\n",
        "    if normMean:\n",
        "        mean = data.std(axis=axis) \n",
        "        data = np.subtract(data, mean)\n",
        "    if normVar:\n",
        "        stdev = data.std(axis=axis) \n",
        "        data = np.divide(data, stdev)\n",
        "    \n",
        "    return data\n",
        "\n",
        "# temporal derrivatives\n",
        "def add_derrivatives_data(data, delta=True, ddelta=True):   \n",
        "    if delta:\n",
        "        data_delta = np.gradient(data, axis=0)\n",
        "        data = np.concatenate((data, data_delta), axis=-1)\n",
        "    if ddelta:\n",
        "        data_delta = np.gradient(data, axis=0)\n",
        "        data_ddelta = np.gradient(data_delta, axis=0)\n",
        "        data = np.concatenate((data, data_delta, data_ddelta), axis=-1)\n",
        " \n",
        "    return data\n",
        "\n",
        "\n",
        "## Wrapper for assembling dataframe containing frames \n",
        "\n",
        "# assemble dataframe\n",
        "#   = row per frame\n",
        "# - path                    = utterance identifier\n",
        "# - center frame index      = index of center frame for current phoneme\n",
        "# - frame index             = index of frame for current phoneme\n",
        "# - window indices          = window indices \n",
        "def assemble_frame_dataframe(df, window_nframes=5, stride=2, shift=0,\n",
        "                             delta=True, ddelta=False,\n",
        "                             normMean=False, normVar=True):\n",
        "    frame_dfs = []\n",
        "    # iterate over utterances\n",
        "    for _, utt in df.iterrows():\n",
        "        \n",
        "        ## modify feature vector\n",
        "        # utterance data\n",
        "        data = utt['data']\n",
        "        # add temporal derrivatives\n",
        "        data = add_derrivatives_data(data, delta=delta, ddelta=ddelta)\n",
        "        # normalize standard deviation per channel (feature dimension)\n",
        "        data = normalize_data(data, axis=0, normMean=normMean, normVar=normVar)\n",
        "        \n",
        "        ## labels and names\n",
        "        # number of frames in utterance\n",
        "        utt_nframes = data.shape[0]\n",
        "        # utterance name for each frame\n",
        "        utt_names = [utt['utt_name']] * utt_nframes\n",
        "        # phone segmentation to phone per frame in utterance\n",
        "        phnseg = utt['phn_seg']\n",
        "        utt_phns = phnseg_to_phns(phnseg, utt_nframes)\n",
        "        centre_frame_ids = phnseg_to_center_frames(phnseg, utt_nframes)\n",
        "        # - indices of (feature) window per frame in utterance \n",
        "        utt_windows = get_windows_for_utt(utt_nframes, window_nframes=window_nframes, \n",
        "                                          stride=stride, shift=shift)\n",
        "        \n",
        "        # index of last frame\n",
        "        # - correct for occasional mismatch in number of frames and phone labels\n",
        "        eidx = min(utt_nframes, len(utt_phns))\n",
        "        \n",
        "        ## frame dataframe per utterance  \n",
        "        # - row per frame in utterance\n",
        "        # - columns: \"utt_name\", \"frame_idx\", window\", \"data\", \"phn_label\"\n",
        "        frame_df = pd.DataFrame({'utt_name' : utt_names[:eidx],\n",
        "                               'center_frame_idx' : centre_frame_ids[:eidx],\n",
        "                               'frame_idx' : np.arange(utt_nframes)[:eidx],\n",
        "                               'window' :  utt_windows[:eidx],\n",
        "                               'data' : list(data[:eidx]),\n",
        "                               'org_phn_label' : utt_phns[:eidx]})\n",
        "         \n",
        "        frame_dfs.append(frame_df)\n",
        "    \n",
        "    # concatenate frame dataframes per utterance to final frame dataframe \n",
        "    frame_df = pd.concat(frame_dfs, ignore_index=True)\n",
        "    \n",
        "    return frame_df\n",
        "\n",
        "\n",
        "## Phone and label mapping \n",
        "\n",
        "# TIMIT phone label to phone label (using phone_map_file)\n",
        "def get_phn2phn_map(phone_map_file, ocol, icol=0):\n",
        "    phn2phn = {}\n",
        "    with open(phone_map_file) as file:\n",
        "        for line in file:\n",
        "            w = line.strip().split(\"\\t\")\n",
        "            phn2phn[w[icol]] = w[ocol]\n",
        "            \n",
        "    return phn2phn \n",
        "\n",
        "# phone label to numeric label\n",
        "def get_phn2lab_map(phn_labels):\n",
        "    phn_labels = unique_list(phn_labels)\n",
        "    phn2lab_map = {phn : i for i, phn in enumerate(phn_labels) }\n",
        "    lab2phn_map = {i : phn for i, phn in enumerate(phn_labels) }\n",
        "    \n",
        "    return phn2lab_map, lab2phn_map \n",
        "\n",
        "# numeric label (based on 'icol') to other numeric label (based on '0col')\n",
        "def get_lab2lab_dict(phone_map_file, icol, ocol, phn_set=None):\n",
        "    # phn2phn\n",
        "    iphn2ophn = get_phn2phn_map(phone_map_file, ocol, icol)\n",
        "\n",
        "    # phn2lab x2\n",
        "    iphn2lab, _ = get_phn2lab_map(list(iphn2ophn.keys()))\n",
        "    ophn2lab, _ = get_phn2lab_map(list(iphn2ophn.values())) \n",
        "\n",
        "    # lab2lab\n",
        "    ilab2olab = {iphn2lab[k]:ophn2lab[v] for k,v in iphn2ophn.items()}\n",
        "    \n",
        "    return ilab2olab\n",
        "\n",
        "# get unique items in list\n",
        "def unique_list(lst): \n",
        "    # dict perserves order (vs. set)\n",
        "    return list(dict.fromkeys(lst))\n",
        "\n",
        "\n",
        "## Operations on dataframe  {display-mode: \"form\"}\n",
        "\n",
        "# split 'utt_name' column for easy data selection\n",
        "def split_string_column(df, col_dict, sep=\"/\"):  \n",
        "    for key, value in col_dict['target'].items():\n",
        "        df[key] = df.apply(lambda row: row[col_dict['source']].split(sep)[value], axis=1)\n",
        "    \n",
        "    return df \n",
        "\n",
        "# data selection based on regex in columns\n",
        "def filter_regex_dataframe(df, include, exclude=None):\n",
        "    if include is not None:\n",
        "        for key, value in include.items():\n",
        "            if value is not None:\n",
        "                df = df[df[key].str.contains(value)]\n",
        "                \n",
        "    if exclude is not None:\n",
        "        for key, value in exclude.items():\n",
        "            if value is not None:\n",
        "                df = df[~df[key].str.contains(value)]\n",
        "    \n",
        "    return df\n",
        "\n",
        "### Wrapper functions\n",
        "\n",
        "def phone_and_label_mapping(phone_map_file, ocol=1, phn_set=None):\n",
        "\n",
        "    # phone mapping parameters\n",
        "    # ocol -> {0 : TIMIT 61 phone label, 1 : 48 phone label, 2: 39 phone label}\n",
        "\n",
        "    # phone mapping - phn2phn dictionairy = {TIMIT phone label : phone label}\n",
        "    phn2phn = get_phn2phn_map(phone_map_file, ocol=ocol)\n",
        "\n",
        "    # label mapping - phn2lab dictionairy = {phone label : integer label}\n",
        "    if phn_set == None:\n",
        "        phn_set = unique_list([phn for phn in phn2phn.values()]) # all values\n",
        "    \n",
        "    phn2lab, lab2phn = get_phn2lab_map(phn_set)\n",
        "\n",
        "    return phn2phn, phn2lab, lab2phn, phn_set\n",
        "\n",
        "# split in train / validation / test set \n",
        "def utterance_based_data_split(frames_df): \n",
        "\n",
        "    # add columns for easy splitting\n",
        "    col_dict = {'source' : 'utt_name',\n",
        "                'target' : {'dataset' : 0, 'dialect' : 1, \n",
        "                            'speaker' : 2, 'utterance' : 3}}\n",
        "    frames_df = split_string_column(frames_df, col_dict=col_dict, sep='/')\n",
        "\n",
        "    # define split\n",
        "    train_re_incl = {'dataset' : 'train'}\n",
        "    train_re_excl = {'dialect' : 'dr8', 'utterance' : 'sa'}\n",
        "    valid_re_incl = {'dataset' : 'train', 'dialect' : 'dr8'}\n",
        "    valid_re_excl = {'utterance' : 'sa'}\n",
        "    test_re_incl = {'dataset' : 'test'}\n",
        "    test_re_excl = {'utterance' : 'sa'}\n",
        "\n",
        "    # split\n",
        "    train_df = filter_regex_dataframe(frames_df, train_re_incl, train_re_excl)\n",
        "    valid_df = filter_regex_dataframe(frames_df, valid_re_incl, valid_re_excl)\n",
        "    test_df = filter_regex_dataframe(frames_df, test_re_incl, test_re_excl)\n",
        "\n",
        "    return train_df, valid_df, test_df\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Tqystd4sCz",
        "cellView": "form"
      },
      "source": [
        "#@title Pytorch functions\n",
        "\n",
        "### Neural network architecture ###\n",
        "\n",
        "# simple fully-connected feedforward neural network \n",
        "class Dnn(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, hidden_layer_sizes):\n",
        "        super(Dnn, self).__init__()\n",
        "\n",
        "        # attributes\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "\n",
        "        # parameters\n",
        "        layer_sizes = (in_dim, *hidden_layer_sizes, out_dim)\n",
        "        layer_sizes_pairwise = [(layer_sizes[i], layer_sizes[i+1]) for \n",
        "                                 i in range(len(layer_sizes)-1)]\n",
        "\n",
        "        # define architecture\n",
        "        modulelist = nn.ModuleList([])  \n",
        "        for i, (layer_in_size, layer_out_size) in enumerate(layer_sizes_pairwise):\n",
        "            modulelist.append(nn.Linear(layer_in_size, layer_out_size))\n",
        "            if i < len(self.hidden_layer_sizes):\n",
        "                modulelist.append(nn.Sigmoid())\n",
        "\n",
        "        # define network as nn.Sequential\n",
        "        self.net = nn.Sequential(*modulelist)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size = input.size(0)\n",
        "        shape = (batch_size, *self.shape)\n",
        "        out = input.view(shape)\n",
        "        return out\n",
        "\n",
        "\n",
        "# fully-connected neural network with 1D convolutional layer\n",
        "class Cnn1d(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, hidden_layer_sizes,\n",
        "                 kernel_size=(5, ), padding=(2, ), n_filters=100,\n",
        "                 cnn_position=0):\n",
        "        super(Cnn1d, self).__init__()\n",
        "\n",
        "        # attributes\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "\n",
        "        # parameters\n",
        "        layer_sizes = (in_dim, *hidden_layer_sizes, out_dim)\n",
        "        layer_sizes_pairwise = [(layer_sizes[i], layer_sizes[i+1]) for \n",
        "                                 i in range(len(layer_sizes)-1)]\n",
        "\n",
        "        # define architecture\n",
        "        modulelist = nn.ModuleList([])\n",
        "        for i, (layer_in_size, layer_out_size) in enumerate(layer_sizes_pairwise):\n",
        "\n",
        "            if i == cnn_position:\n",
        "                modulelist.append(nn.Conv1d(layer_in_size, n_filters, \n",
        "                                            kernel_size, padding=padding))\n",
        "                modulelist.append(View(shape=(-1, )))\n",
        "                    \n",
        "            else:\n",
        "                modulelist.append(nn.Linear(layer_in_size, layer_out_size))\n",
        "\n",
        "            if i < len(self.hidden_layer_sizes):\n",
        "                modulelist.append(nn.Sigmoid())\n",
        "\n",
        "        # define network as nn.Sequential\n",
        "        self.net = nn.Sequential(*modulelist)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "def init_weights(m, bias=0.00):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(bias)\n",
        "\n",
        "\n",
        "# fully-connected neural network with 1D convolutional layer\n",
        "class Cnn2d(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, hidden_layer_sizes,\n",
        "                 kernel_size=(5, 5), padding=(2, 2), n_filters=25,\n",
        "                 cnn_position=0):\n",
        "        super(Cnn2d, self).__init__()\n",
        "\n",
        "        # attributes\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "\n",
        "        # parameters\n",
        "        layer_sizes = (in_dim, *hidden_layer_sizes, out_dim)\n",
        "        layer_sizes_pairwise = [(layer_sizes[i], layer_sizes[i+1]) for \n",
        "                                 i in range(len(layer_sizes)-1)]\n",
        "\n",
        "        # define architecture\n",
        "        modulelist = nn.ModuleList([])\n",
        "        for i, (layer_in_size, layer_out_size) in enumerate(layer_sizes_pairwise):\n",
        "\n",
        "            if i == cnn_position:\n",
        "                modulelist.append(nn.Conv2d(layer_in_size, n_filters, \n",
        "                                            kernel_size, padding=padding))\n",
        "                modulelist.append(View(shape=(-1, )))\n",
        "                    \n",
        "            else:\n",
        "                modulelist.append(nn.Linear(layer_in_size, layer_out_size))\n",
        "\n",
        "            if i < len(self.hidden_layer_sizes):\n",
        "                modulelist.append(nn.Sigmoid())\n",
        "\n",
        "        # define network as nn.Sequential\n",
        "        self.net = nn.Sequential(*modulelist)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "### Neural network training ###\n",
        "\n",
        "# simple batch gradient descent \n",
        "def train_batch(network, train_X, train_y, criterion, optimizer, device, \n",
        "                n_epochs=500, every=10):\n",
        "\n",
        "    # data-format is torch tensor + send to device (ex. GPU)\n",
        "    if not torch.is_tensor(train_X):\n",
        "        train_X = torch.tensor(train_X, dtype=torch.float32).to(device)\n",
        "    if not torch.is_tensor(train_y):\n",
        "        train_y = torch.tensor(train_y, dtype=torch.long).to(device)\n",
        "\n",
        "    # send network to device (ex. GPU)\n",
        "    network.to(device)\n",
        "\n",
        "    # set network to training mode (vs. evaluation mode)\n",
        "    network.train() \n",
        "\n",
        "    # save training loss\n",
        "    train_loss = []\n",
        "\n",
        "    # train for some epochs - full batch\n",
        "    for epoch in range(n_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = network(train_X)\n",
        "        loss = criterion(output, train_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch%every == 0:\n",
        "          print('Epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "        \n",
        "        train_loss.append(loss.item())\n",
        "      \n",
        "    return train_loss\n",
        "            \n",
        "# mini-batch gradient descent with early stopping\n",
        "def train(network, train_dl, criterion, optimizer, device, n_epochs=500, \n",
        "          valid_X=None, valid_y=None, patience=5, every=10, \n",
        "          current_epoch=0):\n",
        "\n",
        "    # set early stopping counter\n",
        "    cnt_valid_loss_increase = 0\n",
        "\n",
        "    # current_epoch\n",
        "    last_epoch = current_epoch\n",
        "\n",
        "    # data-format is torch tensor + send to device (ex. GPU)\n",
        "    if not torch.is_tensor(valid_X):\n",
        "        valid_X = torch.tensor(valid_X, dtype=torch.float32).to(device)\n",
        "    if not torch.is_tensor(valid_y):\n",
        "        valid_y = torch.tensor(valid_y, dtype=torch.long).to(device)\n",
        "\n",
        "    # save training and validation loss\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "\n",
        "    #  per epoch: update network parameters \n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # network to training mode\n",
        "        network.train()\n",
        "        \n",
        "        # early stopping \n",
        "        earlystoppping = cnt_valid_loss_increase > patience\n",
        "        if earlystoppping:\n",
        "            print(\"stopped early after %d epochs\" % (epoch))\n",
        "            return train_loss_list, valid_loss_list, last_epoch\n",
        "\n",
        "        # per mini-batch: compute gradient (backpropagation) + take step \n",
        "        running_loss = 0\n",
        "        steps = 0 \n",
        "        for i, data in enumerate(train_dl, 0):\n",
        "\n",
        "            # mini-batch inputs and labels\n",
        "            frames, labels = data  \n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # forward + backward + optimize \n",
        "            outputs = network.net(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()  \n",
        "\n",
        "            # running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # training loss\n",
        "        mean_minibatch_loss = running_loss/len(train_dl)\n",
        "        train_loss_list.append(mean_minibatch_loss)\n",
        "        if epoch%every == 0:   \n",
        "            print(\"Epoch %d -- av. loss per mini-batch %.2f\" % (epoch, mean_minibatch_loss))\n",
        "\n",
        "        # validation loss\n",
        "        if valid_X is not None and valid_y is not None: \n",
        "            # network to evaluation mode\n",
        "            network.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_outputs = network.net(valid_X)\n",
        "                valid_loss = criterion(valid_outputs, valid_y)\n",
        "                valid_loss_list.append(valid_loss.item())\n",
        "                \n",
        "                # early stoppping\n",
        "                loss_increase = valid_loss_list[-1] > min(valid_loss_list[-patience:])\n",
        "                if loss_increase:\n",
        "                    # no improvement compared to last 'patience' steps\n",
        "                    cnt_valid_loss_increase += 1\n",
        "                else:\n",
        "                    cnt_valid_loss_increase = 0\n",
        "\n",
        "        last_epoch = current_epoch + epoch + 1\n",
        "\n",
        "    return train_loss_list, valid_loss_list, last_epoch\n",
        "\n",
        "\n",
        "### Neural network saving and loading ###\n",
        "\n",
        "# save model\n",
        "def save_model(epoch, model, criterion, optimizer, model_file):\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'criterion': criterion,\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, model_file)\n",
        "    \n",
        "    doPrint = False\n",
        "    if doPrint:\n",
        "        print('Saved model to: ' + model_file)\n",
        "\n",
        "# load model\n",
        "def load_model(model, optimizer, model_file, device, strict=False):\n",
        "    doPrint = False\n",
        "    if doPrint:\n",
        "        print('Loading: ' + model_file)\n",
        "        \n",
        "    checkpoint = torch.load(model_file, map_location=device)\n",
        "    epoch = checkpoint['epoch']    \n",
        "    model.load_state_dict(checkpoint['model_state_dict'], strict=strict)\n",
        "    criterion = checkpoint['criterion']\n",
        "    if doPrint:\n",
        "        print(checkpoint['optimizer_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    return epoch, model, criterion, optimizer\n",
        "\n",
        "### Neural network evaluation ###\n",
        "\n",
        "# evaluate criterion\n",
        "def evaluate_criterion(network, test_X, test_y, criterion, device):\n",
        "    \n",
        "    # data-format is torch tensor + send to device (ex. GPU)\n",
        "    if not torch.is_tensor(test_X):\n",
        "        test_X = torch.tensor(test_X, dtype=torch.float32).to(device)\n",
        "    if not torch.is_tensor(test_y):\n",
        "        test_y = torch.tensor(test_y, dtype=torch.long).to(device)\n",
        "\n",
        "    # send network to device (ex. GPU)\n",
        "    network.to(device)\n",
        "\n",
        "    # set network to evaluation mode (vs. training mode)\n",
        "    network.eval() \n",
        "\n",
        "    # compute loss based on criterion\n",
        "    pred_y = network.net(test_X)\n",
        "    loss = criterion(pred_y, test_y)\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# evaluate confusion matrix\n",
        "def confusionmatrix(network, test_X, test_y, device, \n",
        "                    n_per_pass=2**10, unsqueeze=False, unsqueeze2d=False):\n",
        "    \n",
        "    # data-format is torch tensor + send to device (ex. GPU)\n",
        "    if not torch.is_tensor(test_X):\n",
        "        test_X = torch.tensor(test_X, dtype=torch.float32).to(device)\n",
        "    if not torch.is_tensor(test_y):\n",
        "        test_y = torch.tensor(test_y, dtype=torch.long).to(device)\n",
        "\n",
        "    # send network to device (ex. GPU)\n",
        "    network.to(device)\n",
        "\n",
        "    # set network to evaluation mode (vs. training mode)\n",
        "    network.eval() \n",
        "\n",
        "    # compute confusion matrix\n",
        "    cm = np.zeros((network.out_dim, network.out_dim))\n",
        "        \n",
        "    # splitting up dataframe in managable chunks\n",
        "    n_samples = len(test_X)\n",
        "    loop_n_times = math.ceil(n_samples/n_per_pass)\n",
        "    for i in range(loop_n_times):\n",
        "      test_X_part = test_X[i*n_per_pass:(i+1)*n_per_pass]\n",
        "      test_y_part = test_y[i*n_per_pass:(i+1)*n_per_pass]\n",
        "      for input, label in zip(test_X_part, test_y_part):\n",
        "        if unsqueeze: prob = network.net(torch.unsqueeze(input, 0))\n",
        "        elif unsqueeze2d: prob = network.net(torch.reshape(input, (1, *input.shape)))\n",
        "        else: prob = network.net(input) # output = posterior class probabilities\n",
        "        pred = torch.argmax(prob) # prediction = label (or neuron) with highest probability (One-Hot Encoding)\n",
        "        cm[label][pred] += 1\n",
        "    \n",
        "    return cm.astype(int)\n",
        "\n",
        "\n",
        "# remap confusion matrix\n",
        "def remap_confusionmatrix(confusionmatrix, lab2lab_dict):\n",
        "    \n",
        "    n_classes = confusionmatrix.shape[0]\n",
        "    n_classes_new = len(np.unique((np.fromiter(lab2lab_dict.values(), dtype=int))))\n",
        "    confusionmatrix_new = np.zeros((n_classes_new, n_classes_new))\n",
        "    \n",
        "    # remap classes\n",
        "    for row in range(n_classes):\n",
        "        for col in range(n_classes):\n",
        "            confusionmatrix_new[lab2lab_dict[row], lab2lab_dict[col]] += confusionmatrix[row,col]\n",
        "            \n",
        "    return confusionmatrix_new\n",
        "\n",
        "# evaluate phone error rate\n",
        "def evaluate_PER(confusionmatrix):\n",
        "\n",
        "    n_classes = confusionmatrix.shape[0]    \n",
        "    \n",
        "    # compute ER \n",
        "    trace = np.trace(confusionmatrix)\n",
        "    ER = 1- trace.sum() / confusionmatrix.sum()\n",
        "        \n",
        "    # compute ER per class (disregarding non label or not)\n",
        "    no_examples_pc = confusionmatrix.sum(axis=1)\n",
        "    ER_pc = [None] * n_classes\n",
        "    for i in range(n_classes):\n",
        "        if no_examples_pc[i] != 0:\n",
        "            ER_pc[i] = 1-confusionmatrix[i,i] / (no_examples_pc[i])\n",
        "        \n",
        "    return ER, ER_pc\n",
        "\n",
        "\n",
        "### Confusion matrix plot ###\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# pretty printing\n",
        "def plot_confusion_matrix(cm, labels=[], cmap=[], figsize=(6,6), fontsize=14):\n",
        "\n",
        "    if len(labels) == 0:\n",
        "        df_cm = pd.DataFrame(cm)\n",
        "    else:\n",
        "        df_cm = pd.DataFrame(cm, labels, labels)\n",
        "        \n",
        "    f, ax = plt.subplots(figsize=figsize)\n",
        "    sns.set(font_scale=1.4)#for label size\n",
        "    sns.heatmap(df_cm, annot=True, fmt=\"d\", annot_kws={\"fontsize\": fontsize,\"color\":'k'},\n",
        "                square=True, linecolor='k', linewidth=1.5, cmap=cmap, cbar=False)\n",
        "    ax.tick_params(axis='y', labelrotation=0.0, left=True)\n",
        "    # font size\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "### Dataset ###\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    \"\"\"Simple dataset for easy sampling.\"\"\"\n",
        "\n",
        "    def __init__(self, data_X, data_y, labels, labeldict, device):\n",
        "\n",
        "        # dimensionality\n",
        "        self.n_samples, self.n_features = data_X.shape[0:2]\n",
        "        self.n_classes = len(labels)\n",
        "\n",
        "        # input data\n",
        "        self.frames = data_X # (n_samples, n_features)\n",
        "        self.frames = torch.as_tensor(self.frames, dtype=torch.float32).to(device)\n",
        "\n",
        "        # labels\n",
        "        if data_y.dtype != \"int64\":\n",
        "            data_y = np.vectorize(labeldict.get)(data_y)\n",
        "        self.labels = torch.as_tensor(data_y, dtype=torch.long).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame = self.frames[idx] \n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        return frame, label\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqHgOBsz8qxq",
        "cellView": "form"
      },
      "source": [
        "#@title Define setups\n",
        "\n",
        "def prepare_baseline(utt_df, utt_names=None, phone_map_file='phones-61-48-39.txt'):\n",
        "    \n",
        "    ## DATA SELECTION -- utterance based (speed up)\n",
        "    if utt_names is not None:\n",
        "        utt_df = utt_df[utt_df['utt_name'].isin(utt_names)]\n",
        "\n",
        "    ## FEATURE EXTRACTION\n",
        "\n",
        "    # input feature parameters\n",
        "    window_nframes = 11\n",
        "    stride = 2\n",
        "    shift = 0\n",
        "    delta, ddelta = False, False\n",
        "    normMean, normVar = False, True\n",
        "\n",
        "    # generate dataframe containing frames\n",
        "    frames_df = assemble_frame_dataframe(utt_df, window_nframes, stride, shift, \\\n",
        "                                        delta, ddelta, normMean, normVar)\n",
        "\n",
        "    # assemble input feature from window\n",
        "    features = apply_window(frames_df['data'], frames_df.index, frames_df['window'])\n",
        "    #frames_df['feature'] = features\n",
        "    frames_df['feature'] = reshape_features(features, (-1,)) \n",
        "\n",
        "    ## LABELS\n",
        "\n",
        "    # phone and label mapping\n",
        "    ocol = 1 # {'61' : 0, '48' : 1, '39' : 2}\n",
        "    phn_set = None\n",
        "    phn2phn, phn2lab, lab2phn, phn_set = phone_and_label_mapping(phone_map_file, ocol, phn_set)\n",
        "\n",
        "    # map phones and labels\n",
        "    frames_df['phn_label'] = frames_df['org_phn_label'].map(phn2phn) # TIMIT phone label -> phone label\n",
        "    frames_df['int_label'] = frames_df['phn_label'].map(phn2lab)  # phone label ->  integer label\n",
        "\n",
        "    ## DATA SELECTION -- frame based\n",
        "\n",
        "    # drop phones not in phn_set\n",
        "    frames_df = frames_df[~frames_df['int_label'].isna()] \n",
        "\n",
        "    # drop phones with large shift in phn_set\n",
        "    doShift = False\n",
        "    max_shift = 5 # shift = distance to center frame\n",
        "    if doShift:  \n",
        "      frames_df['shift'] = frames_df['frame_idx'] - frames_df['center_frame_idx']\n",
        "      frames_df = frames_df[frames_df['shift'].abs() < max_shift ]\n",
        "\n",
        "    return frames_df\n",
        "\n",
        "def prepare_shortvowels(utt_df, utt_names=None, phone_map_file='phones-61-48-39.txt',\n",
        "                        phn_set=['aa', 'ae', 'ah', 'ao', 'eh', 'ih', 'ix', 'uh']):\n",
        "    \n",
        "    ## DATA SELECTION -- utterance based (speed up)\n",
        "    if utt_names is not None:\n",
        "        utt_df = utt_df[utt_df['utt_name'].isin(utt_names)]\n",
        "\n",
        "    ## FEATURE EXTRACTION\n",
        "\n",
        "    # input feature parameters\n",
        "    window_nframes = 11\n",
        "    stride = 2\n",
        "    shift = 0\n",
        "    delta, ddelta = False, False\n",
        "    normMean, normVar = False, True\n",
        "\n",
        "    # generate dataframe containing frames\n",
        "    frames_df = assemble_frame_dataframe(utt_df, window_nframes, stride, shift, \\\n",
        "                                        delta, ddelta, normMean, normVar)\n",
        "\n",
        "    # assemble input feature from window\n",
        "    features = apply_window(frames_df['data'], frames_df.index, frames_df['window'])\n",
        "    frames_df['feature'] = reshape_features(features, (-1,)) \n",
        "\n",
        "    ## LABELS\n",
        "\n",
        "    # phone and label mapping\n",
        "    ocol = 1 # {'61' : 0, '48' : 1, '39' : 2}\n",
        "    #phn_set =  ['aa', 'ih']#['aa', 'ae', 'ah', 'ao', 'eh', 'ih', 'ix', 'uh'] \n",
        "    phn2phn, phn2lab, lab2phn, phn_set = phone_and_label_mapping(phone_map_file, ocol, phn_set)\n",
        "\n",
        "    # map phones and labels\n",
        "    frames_df['phn_label'] = frames_df['org_phn_label'].map(phn2phn) # TIMIT phone label -> phone label\n",
        "    frames_df['int_label'] = frames_df['phn_label'].map(phn2lab)  # phone label ->  integer label\n",
        "\n",
        "    ## DATA SELECTION -- frame based\n",
        "\n",
        "    # drop phones not in phn_set\n",
        "    frames_df = frames_df[~frames_df['int_label'].isna()] \n",
        "\n",
        "    # drop phones with large shift in phn_set\n",
        "    doShift = False\n",
        "    max_shift = 5 # shift = distance to center frame\n",
        "    if doShift:  \n",
        "      frames_df['shift'] = frames_df['frame_idx'] - frames_df['center_frame_idx']\n",
        "      frames_df = frames_df[frames_df['shift'].abs() < max_shift ]\n",
        "\n",
        "    return frames_df\n",
        "\n",
        "def prepare_cnn(utt_df, utt_names=None, phone_map_file='phones-61-48-39.txt'):\n",
        "    \n",
        "    ## DATA SELECTION -- utterance based (speed up)\n",
        "    if utt_names is not None:\n",
        "        utt_df = utt_df[utt_df['utt_name'].isin(utt_names)]\n",
        "\n",
        "    ## FEATURE EXTRACTION\n",
        "\n",
        "    # input feature parameters\n",
        "    window_nframes = 11\n",
        "    stride = 2\n",
        "    shift = 0\n",
        "    delta, ddelta = False, False\n",
        "    normMean, normVar = False, True\n",
        "\n",
        "    # generate dataframe containing frames\n",
        "    frames_df = assemble_frame_dataframe(utt_df, window_nframes, stride, shift, \\\n",
        "                                        delta, ddelta, normMean, normVar)\n",
        "\n",
        "    # assemble input feature from window\n",
        "    features = apply_window(frames_df['data'], frames_df.index, frames_df['window'])\n",
        "    frames_df['feature'] = reshape_features(features, (-1,)) \n",
        "\n",
        "    ## LABELS\n",
        "\n",
        "    # phone and label mapping\n",
        "    ocol = 1 # {'61' : 0, '48' : 1, '39' : 2}\n",
        "    phn_set = None\n",
        "    phn2phn, phn2lab, lab2phn, phn_set = phone_and_label_mapping(phone_map_file, ocol, phn_set)\n",
        "\n",
        "    # map phones and labels\n",
        "    frames_df['phn_label'] = frames_df['org_phn_label'].map(phn2phn) # TIMIT phone label -> phone label\n",
        "    frames_df['int_label'] = frames_df['phn_label'].map(phn2lab)  # phone label ->  integer label\n",
        "\n",
        "    ## DATA SELECTION -- frame based\n",
        "\n",
        "    # drop phones not in phn_set\n",
        "    frames_df = frames_df[~frames_df['int_label'].isna()] \n",
        "\n",
        "    # drop phones with large shift in phn_set\n",
        "    doShift = False\n",
        "    max_shift = 5 # shift = distance to center frame\n",
        "    if doShift:  \n",
        "      frames_df['shift'] = frames_df['frame_idx'] - frames_df['center_frame_idx']\n",
        "      frames_df = frames_df[frames_df['shift'].abs() < max_shift ]\n",
        "\n",
        "\n",
        "## Custom setups\n",
        "\n",
        "f11s2_setup = {'window_nframes' : 11, 'stride' : 2, 'shift' : 0,\n",
        "               'delta' : False, 'ddelta' : False,\n",
        "               'normMean' : False, 'normVar' : True}\n",
        "\n",
        "f7s2_setup = {'window_nframes' : 7, 'stride' : 2, 'shift' : 0,\n",
        "               'delta' : False, 'ddelta' : False,\n",
        "               'normMean' : False, 'normVar' : True}\n",
        "\n",
        "f7s2d_setup = {'window_nframes' : 7, 'stride' : 1, 'shift' : 0,\n",
        "               'delta' : True, 'ddelta' : False,\n",
        "               'normMean' : False, 'normVar' : True}\n",
        "\n",
        "f13s1_setup = {'window_nframes' : 13, 'stride' : 1, 'shift' : 0,\n",
        "               'delta' : False, 'ddelta' : False,\n",
        "               'normMean' : False, 'normVar' : True}\n",
        "\n",
        "f13s2_setup = {'window_nframes' : 13, 'stride' : 2, 'shift' : 0,\n",
        "               'delta' : False, 'ddelta' : False,\n",
        "               'normMean' : False, 'normVar' : True}\n",
        "\n",
        "\n",
        "def prepare_setup(utt_df, setup, utt_names=None, ocol=1, phn_set=None,  \n",
        "                  phone_map_file='phones-61-48-39.txt'):\n",
        "    \n",
        "    ## DATA SELECTION -- utterance based (speed up)\n",
        "    if utt_names is not None:\n",
        "        utt_df = utt_df[utt_df['utt_name'].isin(utt_names)]\n",
        "\n",
        "    ## FEATURE EXTRACTION\n",
        "\n",
        "    # input feature parameters\n",
        "    window_nframes = setup['window_nframes']\n",
        "    stride = setup['stride']\n",
        "    shift = setup['shift']\n",
        "    delta, ddelta = setup['delta'], setup['ddelta']\n",
        "    normMean, normVar = setup['normMean'], setup['normVar']\n",
        "\n",
        "    # generate dataframe containing frames\n",
        "    frames_df = assemble_frame_dataframe(utt_df, window_nframes, stride, shift, \\\n",
        "                                        delta, ddelta, normMean, normVar)\n",
        "\n",
        "    # assemble input feature from window\n",
        "    features = apply_window(frames_df['data'], frames_df.index, frames_df['window'])\n",
        "    frames_df['feature'] = reshape_features(features, (-1,)) \n",
        "\n",
        "    ## LABELS\n",
        "\n",
        "    # phone and label mapping\n",
        "    phn2phn, phn2lab, lab2phn, phn_set = phone_and_label_mapping(phone_map_file, ocol, phn_set)\n",
        "\n",
        "    # map phones and labels\n",
        "    frames_df['phn_label'] = frames_df['org_phn_label'].map(phn2phn) # TIMIT phone label -> phone label\n",
        "    frames_df['int_label'] = frames_df['phn_label'].map(phn2lab)  # phone label ->  integer label\n",
        "\n",
        "    ## DATA SELECTION -- frame based\n",
        "\n",
        "    # drop phones not in phn_set\n",
        "    frames_df = frames_df[~frames_df['int_label'].isna()] \n",
        "\n",
        "    # drop phones with large shift in phn_set\n",
        "    doShift = False\n",
        "    max_shift = 5 # shift = distance to center frame\n",
        "    if doShift:  \n",
        "      frames_df['shift'] = frames_df['frame_idx'] - frames_df['center_frame_idx']\n",
        "      frames_df = frames_df[frames_df['shift'].abs() < max_shift ]\n",
        "\n",
        "    return frames_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIjZCTLg41mf"
      },
      "source": [
        "### 2. The TIMIT database \n",
        "The experiments in this notebook use the TIMIT database.\n",
        "Instead of starting from raw speech, we already extracted features (24 filterbank energies) from the speech signal. The feature is obtained by applying the FFT transform (**window step** = 10ms, window size = 30ms, window = Hamming, ...) and mel scaling to the speech waveform (16kHz). Note the window step determines the frame duration (10ms).\n",
        "\n",
        "The dataframe contains for each utterance:\n",
        "- utt_name : utterance name\n",
        "- data : matrix of # frames in utterance x 24 mel-scaled filterbank energies \n",
        "- phn_seg : segmentation of utterance into phones\n",
        "- text: transcription \n",
        "\n",
        "The phone mapping translates the 61 phone labels used by TIMIT to a smaller phone set with 48 (or 39) phone labels.\n",
        "For example, the 61 phone set discriminates between different types of silences (bcl, dcl, epi, h#, ...), whereas the 39 phone set maps all these to a single silence-label (sil).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFGXk5A9cS1a"
      },
      "source": [
        "# download dataframe \n",
        "root_url = 'http://homes.esat.kuleuven.be/~spchlab/data/timit/'\n",
        "utt_df_url = root_url + 'examples/timit-fb24-100Hz-text.pkl' \n",
        "utt_df = pd.read_pickle(utt_df_url) \n",
        "\n",
        "# download phone mapping\n",
        "phone_map_file = 'phones-61-48-39.txt'\n",
        "phone_map_file_url = root_url + 'examples/phones-61-48-39.txt'\n",
        "write_from_url(phone_map_file_url, phone_map_file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7nYmdkg0SQy"
      },
      "source": [
        "f11s2_setup = {'window_nframes' : 11, 'stride' : 2, 'shift' : 0,\n",
        "               'delta' : False, 'ddelta' : False,\n",
        "               'normMean' : False, 'normVar' : True} "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AbGPiN70Kt0"
      },
      "source": [
        "### DNN vs. GMM on short vowels\n",
        "\n",
        "Last exercise session on phoneme classification, we compared the performance of DNNs and GMMs. Near the end we remarked the toy problem was too small for meaningful conclusions. Here we compare the two models on a subset of TIMIT, namely the short vowels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhOYltapZNje"
      },
      "source": [
        "# subset of phone labels\n",
        "long_vowels = ['aw', 'ay', 'er', 'ey', 'iy', 'ow', 'oy', 'uw']\n",
        "short_vowels = ['aa', 'ao', 'ae', 'ah', 'ax', 'eh', 'ih', 'ix', 'uh']\n",
        "\n",
        "# use a subset of phones\n",
        "phn_set = short_vowels # short_vowels # long_vowels # None # \n",
        "\n",
        "# compute phone and label mapping\n",
        "ocol = 1 # {0 :'61 phones' : 0, 1 : '48 phones', 2 : '39 phones'}\n",
        "phn2phn, phn2lab, lab2phn, phn_set = phone_and_label_mapping(phone_map_file, ocol, phn_set)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uUzllKNVPqE"
      },
      "source": [
        "#### Feature extraction\n",
        "\n",
        "We use the same feature extraction setup ('f11s2_setup') as above.\n",
        "Since we are only using the short vowels of TIMIT, examples with other labels are discarted during the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OznF20WIPoFv"
      },
      "source": [
        "# Prepare train/validation/test set\n",
        "# ! takes around 2 minutes (~ combining frames in context window)\n",
        "shortvowels_df = prepare_setup(utt_df, f11s2_setup, phn_set=phn_set)\n",
        "\n",
        "# phone and label mapping\n",
        "phn2phn, phn2lab, lab2phn, phn_set = phone_and_label_mapping(phone_map_file, ocol, phn_set)\n",
        "\n",
        "# split data\n",
        "train_df, valid_df, test_df = utterance_based_data_split(shortvowels_df)\n",
        "\n",
        "# input features\n",
        "dnn_train_X = np.vstack(train_df['feature']).astype('float32') \n",
        "dnn_valid_X = np.vstack(valid_df['feature']).astype('float32') \n",
        "dnn_test_X = np.vstack(test_df['feature']).astype('float32') \n",
        "\n",
        "# labels\n",
        "train_y = train_df['int_label'].to_numpy().astype('int64') \n",
        "valid_y = valid_df['int_label'].to_numpy().astype('int64')\n",
        "test_y = test_df['int_label'].to_numpy().astype('int64')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3UwpkkosV2D"
      },
      "source": [
        "#### GMM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIiegoHbbCFL",
        "cellView": "form"
      },
      "source": [
        "#@title GMM functions\n",
        "\n",
        "# =============================================================================\n",
        "# GMM\n",
        "# =============================================================================\n",
        "\n",
        "def train_test_GMM(X_train,X_test,y_train,y_test,classes,\n",
        "                     n_components=1,max_iter=20,tol=1.e-3,print_result=True,print_cmat=False):\n",
        "    clf_GM = GaussianMixtureClf(classes=classes,n_components=n_components,max_iter=20,tol=1.e-3)\n",
        "    clf_GM.fit(X_train,y_train)\n",
        "    y_pred = clf_GM.predict(X_train)\n",
        "    acc_train = 100.0*skmetrics.accuracy_score(y_train, y_pred)\n",
        "    y_pred = clf_GM.predict(X_test)\n",
        "    acc_test = 100.0*skmetrics.accuracy_score(y_test, y_pred) \n",
        "    cmat = skmetrics.confusion_matrix(y_test,y_pred)\n",
        "    if(print_result):\n",
        "        lls, bics = llscore(clf_GM,X_train,y_train)\n",
        "        print('Training Set:  Accuracy = %.2f%%     LL = %.2f    BIC = %.2f ' % (acc_train,lls,bics) )\n",
        "        print('Test Set:      Accuracy = %.2f%%'  % (acc_test) )\n",
        "    if(print_cmat):\n",
        "        plot_confusion_matrix(cmat,labels=classes)\n",
        "    return (acc_test,acc_train)\n",
        "\n",
        "def llscore(GMM,X,y):\n",
        "    ''' Average log likelihood per sample over the full data set (X,y) \n",
        "    and BIC per sample '''\n",
        "    ll = 0.\n",
        "    for k in range(0,GMM.n_classes) :\n",
        "        ll += GMM.gmm[k].score(X[y== GMM.classes[k],: ])\n",
        "    lls = ll.mean()\n",
        "    nparam = ((2*n_dim+1)*n_components -1 ) * GMM.n_classes\n",
        "    bics = -2*lls + (np.log(X.shape[0])* nparam) / float(X.shape[0])\n",
        "    return(lls,bics)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfCE37DfIkNM"
      },
      "source": [
        "##### Input feature: window of frames "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B83uHHLgSar_",
        "outputId": "62f38de6-e860-46f8-a5ab-da716ea5a43a"
      },
      "source": [
        "print(dnn_train_X.shape)\n",
        "print(dnn_test_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test_y.shape)\n",
        "print(dnn_train_X.ndim)\n",
        "print(dnn_test_X.ndim)\n",
        "print(train_y.ndim)\n",
        "print(test_y.ndim)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(215588, 264)\n",
            "(82256, 264)\n",
            "(215588,)\n",
            "(82256,)\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "R7ZPLCi-Jj-U",
        "outputId": "7d58393c-ba69-4d51-8e6c-3552a5a0982b"
      },
      "source": [
        "# GMM model\n",
        "n_dim = dnn_train_X.shape[1] #  feature dimension\n",
        "n_components = 5 # n_components to test\n",
        "classes = phn_set\n",
        "\n",
        "print(\"***n_components=%d ***\" % (n_components))\n",
        "_,_ = train_test_GMM(dnn_train_X,dnn_test_X,train_y,test_y,classes,\n",
        "                     n_components=n_components,print_result=True,print_cmat=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***n_components=5 ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspch/GaussianMixtureClf.py:90: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  selection = (y== self.classes[k])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7082fb071fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***n_components=%d ***\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m _,_ = train_test_GMM(dnn_train_X,dnn_test_X,train_y,test_y,classes,\n\u001b[0;32m----> 8\u001b[0;31m                      n_components=n_components,print_result=True,print_cmat=True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-785e94418954>\u001b[0m in \u001b[0;36mtrain_test_GMM\u001b[0;34m(X_train, X_test, y_train, y_test, classes, n_components, max_iter, tol, print_result, print_cmat)\u001b[0m\n\u001b[1;32m      8\u001b[0m                      n_components=1,max_iter=20,tol=1.e-3,print_result=True,print_cmat=False):\n\u001b[1;32m      9\u001b[0m     \u001b[0mclf_GM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixtureClf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclf_GM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_GM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mskmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspch/GaussianMixtureClf.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#            print(\"Model for class: \",self.gmm[k].means_,np.sqrt(self.gmm[k].covariances_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mmixture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mComponent\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    786\u001b[0m             raise ValueError(\n\u001b[1;32m    787\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             )\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjGAKXI7I0mU"
      },
      "source": [
        "##### Input feature: single frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbgESBS5PwzP"
      },
      "source": [
        "# single frame features\n",
        "sframe_train_X = np.vstack(train_df['data']).astype('float32') \n",
        "sframe_test_X = np.vstack(test_df['data']).astype('float32') "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "hkvw_-szSLbT",
        "outputId": "48a10939-aa5f-4e6f-bda6-059a385664fa"
      },
      "source": [
        "# GMM model\n",
        "n_dim = sframe_train_X.shape[1] #  feature dimension\n",
        "n_components = 5 #  n_components to test\n",
        "classes = phn_set\n",
        "\n",
        "print(\"***n_components=%d ***\" % (n_components))\n",
        "_,_ = train_test_GMM(sframe_train_X,sframe_test_X,train_y,test_y,classes,\n",
        "                     n_components=n_components,print_result=True,print_cmat=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***n_components=5 ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspch/GaussianMixtureClf.py:90: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  selection = (y== self.classes[k])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-234210b5ee71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***n_components=%d ***\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m _,_ = train_test_GMM(sframe_train_X,sframe_test_X,train_y,test_y,classes,\n\u001b[0;32m----> 8\u001b[0;31m                      n_components=n_components,print_result=True,print_cmat=True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-785e94418954>\u001b[0m in \u001b[0;36mtrain_test_GMM\u001b[0;34m(X_train, X_test, y_train, y_test, classes, n_components, max_iter, tol, print_result, print_cmat)\u001b[0m\n\u001b[1;32m      8\u001b[0m                      n_components=1,max_iter=20,tol=1.e-3,print_result=True,print_cmat=False):\n\u001b[1;32m      9\u001b[0m     \u001b[0mclf_GM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixtureClf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclf_GM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_GM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mskmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspch/GaussianMixtureClf.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#            print(\"Model for class: \",self.gmm[k].means_,np.sqrt(self.gmm[k].covariances_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mmixture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mComponent\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    786\u001b[0m             raise ValueError(\n\u001b[1;32m    787\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             )\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqWavbEsJRBG"
      },
      "source": [
        "##### Input feature: MFCC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1oRPdfGO4UX"
      },
      "source": [
        "# MFCC features\n",
        "mfcc_train_X = dct(sframe_train_X, type=2, axis=1, norm='ortho')\n",
        "mfcc_test_X = dct(sframe_test_X, type=2, axis=1, norm='ortho')\n",
        "\n",
        "# add first order temporal derrivatives for GMM\n",
        "mfcc_train_X = add_derrivatives_data(mfcc_train_X, delta=True, ddelta=False)\n",
        "mfcc_test_X = add_derrivatives_data(mfcc_test_X, delta=True, ddelta=False)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "au-lFp25Oy8q",
        "outputId": "7f603159-dc12-4bb1-b56e-ddb70a5320fb"
      },
      "source": [
        "# GMM model\n",
        "n_dim = mfcc_train_X.shape[1] #  feature dimension\n",
        "n_components = 5 #  n_components to test\n",
        "classes = phn_set\n",
        "\n",
        "print(\"***n_components=%d ***\" % (n_components))\n",
        "_,_ = train_test_GMM(mfcc_train_X,mfcc_test_X,train_y,test_y,classes,\n",
        "                     n_components=n_components,print_result=True,print_cmat=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***n_components=5 ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspch/GaussianMixtureClf.py:90: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  selection = (y== self.classes[k])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4b8c25456049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***n_components=%d ***\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m _,_ = train_test_GMM(mfcc_train_X,mfcc_test_X,train_y,test_y,classes,\n\u001b[0;32m----> 8\u001b[0;31m                      n_components=n_components,print_result=True,print_cmat=True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-785e94418954>\u001b[0m in \u001b[0;36mtrain_test_GMM\u001b[0;34m(X_train, X_test, y_train, y_test, classes, n_components, max_iter, tol, print_result, print_cmat)\u001b[0m\n\u001b[1;32m      8\u001b[0m                      n_components=1,max_iter=20,tol=1.e-3,print_result=True,print_cmat=False):\n\u001b[1;32m      9\u001b[0m     \u001b[0mclf_GM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixtureClf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclf_GM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_GM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mskmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspch/GaussianMixtureClf.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#            print(\"Model for class: \",self.gmm[k].means_,np.sqrt(self.gmm[k].covariances_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mmixture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mComponent\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    786\u001b[0m             raise ValueError(\n\u001b[1;32m    787\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             )\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
          ]
        }
      ]
    }
  ]
}